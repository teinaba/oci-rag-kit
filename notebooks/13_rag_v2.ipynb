{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGå®Ÿè¡Œãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ (v2: ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€FAQãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦RAGå‡¦ç†ã‚’å®Ÿè¡Œã—ã€\n",
    "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã€ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€LLMå›ç­”ç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "**å‰ææ¡ä»¶**:\n",
    "- `.env`ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¨­å®šæ¸ˆã¿ï¼ˆDBæ¥ç¶šã€OCIèªè¨¼æƒ…å ±ï¼‰\n",
    "- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆsource_documents, chunksï¼‰ãŒä½œæˆæ¸ˆã¿\n",
    "- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²æ¸ˆã¿ï¼ˆ12_data_pipeline_v2.ipynbå®Ÿè¡Œæ¸ˆã¿ï¼‰\n",
    "- Object Storageã«FAQãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "###  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ   ###\n",
    "####################################\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ \n",
    "# notebooksãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹å ´åˆã€è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ \n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"âœ“ Project root added to Python path: {project_root}\")\n",
    "\n",
    "\n",
    "####################################\n",
    "########  å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¾¤ ##########\n",
    "####################################\n",
    "\n",
    "# æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Oracle Database\n",
    "import oracledb\n",
    "\n",
    "# è¨­å®šèª­ã¿è¾¼ã¿\n",
    "from config_loader import ConfigLoader\n",
    "\n",
    "# RAGå‡¦ç†ã‚¯ãƒ©ã‚¹\n",
    "from src.rag.vector_searcher import VectorSearcher\n",
    "from src.rag.reranker import JapaneseReranker\n",
    "from src.rag.answer_generator import AnswerGenerator\n",
    "from src.rag.ragas_evaluator import RagasEvaluator\n",
    "from src.rag.excel_handler import ExcelHandler\n",
    "from src.rag.rag_pipeline import RAGPipeline\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è¨­å®šã®èª­ã¿è¾¼ã¿ã¨DBæ¥ç¶š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "#  è¨­å®šã¨æ¥ç¶š  #\n",
    "################\n",
    "\n",
    "# ConfigLoaderã‚’ä½¿ç”¨ã—ã¦è¨­å®šã‚’èª­ã¿è¾¼ã¿\n",
    "config = ConfigLoader()\n",
    "config.load_env()\n",
    "\n",
    "# Oracle Databaseæ¥ç¶š\n",
    "db_params = config.get_db_params()\n",
    "connection = oracledb.connect(**db_params)\n",
    "\n",
    "# OCIè¨­å®š\n",
    "oci_config = config.get_oci_config()\n",
    "\n",
    "# GenAIè¨­å®š\n",
    "genai_config = config.get_genai_config()\n",
    "compartment_id = genai_config['compartment_id']\n",
    "embedding_model = genai_config['embed_model']\n",
    "\n",
    "# Object Storageè¨­å®š\n",
    "os_config = config.get_object_storage_config()\n",
    "bucket_name = os_config['bucket_name']\n",
    "\n",
    "# Object Storageã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆã—ã¦namespaceã‚’å–å¾—\n",
    "os_client = config.get_object_storage_client()\n",
    "namespace = os_client.get_namespace().data\n",
    "\n",
    "# GenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå…±é€šï¼‰\n",
    "genai_client = config.get_genai_client()\n",
    "\n",
    "print(f\"âœ“ OCIãƒªã‚½ãƒ¼ã‚¹ã®æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "print(f\"  - Database: {connection}\")\n",
    "print(f\"  - Bucket: {bucket_name}\")\n",
    "print(f\"  - Namespace: {namespace}\")\n",
    "print(f\"  - Compartment ID: {compartment_id[:20]}...\")\n",
    "print(f\"  - Embedding Model: {embedding_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAGãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦ç·¨é›†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#  RAGãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š  #\n",
    "############################\n",
    "\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "TOP_K = 10  # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ä¸Šä½ä½•ä»¶ã‚’è¿”å´ã™ã‚‹ã‹ã€‚Rerankç„¡åŠ¹æ™‚ã¯ã“ã®å€¤ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»¶æ•°\n",
    "\n",
    "# Rerankã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (Rerankã¯CPUå‡¦ç†æ€§èƒ½ã«ä¾ã£ã¦ã¯é•·æ™‚é–“è¦ã™ã‚‹ãŸã‚æ³¨æ„)\n",
    "RERANK_ENABLED = True  # True or False\n",
    "RERANK_TOP_N = 5  # Rerankå¾Œã«ä¸Šä½ä½•ä»¶ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ã‹\n",
    "\n",
    "# LLMé¸æŠãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "# ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼š\n",
    "# ã€Cohereã€‘ \"cohere.command-a-03-2025\", \"cohere.command-r-plus-08-2024\"\n",
    "# ã€Meta Llamaã€‘ \"meta.llama-3.3-70b-instruct\"\n",
    "# ã€xAI Grokã€‘ \"xai.grok-4-fast-non-reasoning\", \"xai.grok-4-fast-reasoning\", \"xai.grok-4\"\n",
    "# ã€Google Geminiã€‘ \"google.gemini-2.5-pro\", \"google.gemini-2.5-flash\", \"google.gemini-2.5-flash-lite\"\n",
    "# ã€OpenAI GPT-OSSã€‘ \"openai.gpt-oss-20b\", \"openai.gpt-oss-120b\"\n",
    "CHAT_MODEL = \"cohere.command-a-03-2025\"  # ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š\n",
    "\n",
    "# LLMãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "MAX_TOKENS = 1000  # ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "TEMPERATURE = 0.3  # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0-1ã€å‰µé€ æ€§ã®åˆ¶å¾¡ã€‚0ã«è¿‘ã„ã»ã©æ±ºå®šçš„ã€1ã«è¿‘ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ï¼‰\n",
    "TOP_P = 0.75  # Nucleus samplingã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0-1ï¼‰\n",
    "FREQUENCY_PENALTY = 0.0  # é »åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆCohereç”¨ã€0-1ã€‚ç¹°ã‚Šè¿”ã—ã‚’æŠ‘åˆ¶ï¼‰\n",
    "TOP_K_SAMPLING = 0  # Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆCohereç”¨ã€0ã§ç„¡åŠ¹ï¼‰\n",
    "\n",
    "# å›ç­”ç”Ÿæˆæ™‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "ANSWER_PROMPT = \"\"\"\n",
    "å‚è€ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æƒ…å ±ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚\n",
    "å›ç­”ã¯ç°¡æ½”ã«å¹³æ–‡ã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# FAQãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: faq.xlsxï¼‰\n",
    "FAQ_OBJECT_NAME = config.get_faq_object_name()\n",
    "\n",
    "print(f\"âœ“ RAGãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šå€¤\")\n",
    "print(f\"  - Vector Search TOP_K: {TOP_K}\")\n",
    "print(f\"  - Rerank Enabled: {RERANK_ENABLED}\")\n",
    "print(f\"  - Rerank TOP_N: {RERANK_TOP_N}\")\n",
    "print(f\"  - Chat Model: {CHAT_MODEL}\")\n",
    "print(f\"  - Max Tokens: {MAX_TOKENS}\")\n",
    "print(f\"  - Temperature: {TEMPERATURE}\")\n",
    "print(f\"  - FAQ File: {FAQ_OBJECT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ã¨ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ä½œæˆï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#  RAGã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–  # \n",
    "############################\n",
    "\n",
    "# Phase 1: VectorSearcherï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼‰\n",
    "searcher = VectorSearcher(\n",
    "    db_params=db_params,\n",
    "    embedding_model=embedding_model,\n",
    "    genai_client=genai_client,\n",
    "    compartment_id=compartment_id,\n",
    "    service_endpoint=genai_config['endpoint']\n",
    ")\n",
    "\n",
    "# Phase 2: JapaneseRerankerï¼ˆãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰\n",
    "# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã¨ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡ºã¯è‡ªå‹•\n",
    "print(\"\\nInitializing JapaneseReranker...\")\n",
    "reranker = JapaneseReranker()\n",
    "print(\"âœ“ JapaneseReranker initialized\")\n",
    "\n",
    "# Phase 3: AnswerGeneratorï¼ˆLLMå›ç­”ç”Ÿæˆï¼‰\n",
    "generator = AnswerGenerator(\n",
    "    genai_client=genai_client,\n",
    "    compartment_id=compartment_id\n",
    ")\n",
    "\n",
    "# Phase 4: RagasEvaluatorï¼ˆRAGASè©•ä¾¡ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "# åŸºæœ¬ç‰ˆã§ã¯ None ã«ã—ã¦ã€è©•ä¾¡ã¯åˆ¥ã‚»ãƒ«ã§å®Ÿè¡Œ\n",
    "evaluator = None\n",
    "\n",
    "# Phase 5: ExcelHandlerï¼ˆObject Storage I/Oï¼‰\n",
    "# FAQç”¨ã®ãƒã‚±ãƒƒãƒˆåã‚’å–å¾—ï¼ˆOCI_FAQ_BUCKET_NAMEã¾ãŸã¯OCI_BUCKET_NAMEï¼‰\n",
    "faq_bucket_name = config.get_faq_bucket_name()\n",
    "excel_handler = ExcelHandler(\n",
    "    oci_config=oci_config,\n",
    "    bucket_name=faq_bucket_name,\n",
    "    namespace=namespace\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ All 5 components initialized\")\n",
    "print(f\"  1. VectorSearcher (model: {embedding_model})\")\n",
    "print(f\"  2. JapaneseReranker (model: hotchpotch/japanese-reranker-base-v2)\")\n",
    "print(f\"  3. AnswerGenerator (compartment: {compartment_id[:20]}...)\")\n",
    "print(f\"  4. RagasEvaluator (skipped for now)\")\n",
    "print(f\"  5. ExcelHandler (bucket: {faq_bucket_name}, namespace: {namespace})\")\n",
    "\n",
    "\n",
    "############################\n",
    "#  ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯  # \n",
    "############################\n",
    "\n",
    "def progress_callback(message: str):\n",
    "    \"\"\"\n",
    "    å‡¦ç†é€²æ—ã‚’è¡¨ç¤º\n",
    "    \n",
    "    Args:\n",
    "        message: é€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "    \"\"\"\n",
    "    print(f\"  {message}\")\n",
    "\n",
    "\n",
    "############################\n",
    "#  RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿  # \n",
    "############################\n",
    "\n",
    "# Phase 6: RAGPipelineï¼ˆå…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±åˆï¼‰\n",
    "pipeline = RAGPipeline(\n",
    "    searcher=searcher,\n",
    "    reranker=reranker,\n",
    "    generator=generator,\n",
    "    evaluator=evaluator,  # None\n",
    "    enable_reranking=RERANK_ENABLED,\n",
    "    top_k=TOP_K,\n",
    "    rerank_top_n=RERANK_TOP_N,\n",
    "    progress_callback=progress_callback\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ RAGPipeline orchestrator created\")\n",
    "print(f\"  - Reranking: {'Enabled' if RERANK_ENABLED else 'Disabled'}\")\n",
    "print(f\"  - Vector search top_k: {TOP_K}\")\n",
    "print(f\"  - Rerank top_n: {RERANK_TOP_N}\")\n",
    "print(\"  - Error isolation enabled (individual question failures won't stop processing)\")\n",
    "print(\"  - Progress tracking enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FAQãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#  FAQãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿  # \n",
    "############################\n",
    "\n",
    "# Object Storageã‹ã‚‰FAQãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "faq_df = excel_handler.load_faq(FAQ_OBJECT_NAME)\n",
    "\n",
    "print(f\"âœ“ FAQ file loaded: {len(faq_df)} questions\")\n",
    "print(f\"\\nFAQ Data Preview:\")\n",
    "print(faq_df[['id', 'question', 'ground_truth', 'filter']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAGå‡¦ç†å®Ÿè¡Œï¼ˆå›ç­”ç”Ÿæˆï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# ãƒ¡ã‚¤ãƒ³å‡¦ç† #\n",
    "############\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting RAG Processing...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "print(f\"Total questions: {len(faq_df)}\")\n",
    "print(f\"Model: {CHAT_MODEL}\\n\")\n",
    "\n",
    "# ã™ã¹ã¦ã®è³ªå•ã‚’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†\n",
    "# process_batch()ã¯å„è³ªå•ã‚’é †æ¬¡å‡¦ç†ã—ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶™ç¶š\n",
    "batch_result = pipeline.process_batch(\n",
    "    questions_df=faq_df,\n",
    "    model=CHAT_MODEL,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    frequency_penalty=FREQUENCY_PENALTY,\n",
    "    top_k=TOP_K_SAMPLING,\n",
    "    answer_prompt=ANSWER_PROMPT\n",
    ")\n",
    "\n",
    "# å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing Complete\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“Š Summary:\")\n",
    "print(f\"  âœ“ Success: {batch_result.successful} questions\")\n",
    "print(f\"  âœ— Failed:  {batch_result.failed} questions\")\n",
    "print(f\"  ğŸ“ Total:   {batch_result.total_questions} questions\")\n",
    "print(f\"  â±ï¸ Time:    {batch_result.elapsed_time:.2f} seconds\")\n",
    "print(f\"  ğŸ“ˆ Avg:     {batch_result.elapsed_time/batch_result.total_questions:.2f} seconds/question\")\n",
    "\n",
    "# å‡¦ç†æ™‚é–“ã®çµ±è¨ˆã‚’è¡¨ç¤º\n",
    "results_df = batch_result.results_df\n",
    "print(f\"\\nã€å‡¦ç†æ™‚é–“ã®çµ±è¨ˆã€‘\")\n",
    "print(f\"  ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å¹³å‡: {results_df['vector_search_time'].mean():.2f}ç§’\")\n",
    "print(f\"  Rerankå¹³å‡: {results_df['rerank_time'].mean():.2f}ç§’\")\n",
    "print(f\"  å›ç­”ç”Ÿæˆå¹³å‡: {results_df['generation_time'].mean():.2f}ç§’\")\n",
    "print(f\"  åˆè¨ˆå¹³å‡: {results_df['total_time'].mean():.2f}ç§’\")\n",
    "\n",
    "# çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤º\n",
    "print(f\"\\nğŸ“‹ Results Preview:\")\n",
    "print(results_df[['id', 'question', 'answer', 'total_time', 'status']].head())\n",
    "\n",
    "# å¤±æ•—ã—ãŸè³ªå•ãŒã‚ã‚Œã°è­¦å‘Š\n",
    "if batch_result.failed > 0:\n",
    "    print(f\"\\nâš  Warning: {batch_result.failed} questions failed to process\")\n",
    "    failed_df = results_df[results_df['status'] == 'failed']\n",
    "    print(\"\\nFailed questions:\")\n",
    "    print(failed_df[['id', 'question', 'error']])\n",
    "\n",
    "\n",
    "############################\n",
    "#  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ  # \n",
    "############################\n",
    "\n",
    "# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’å«ã‚€ï¼‰\n",
    "metadata = {\n",
    "    'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿': [\n",
    "        'TOP_K (ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ä»¶æ•°)',\n",
    "        'RERANK_ENABLED (RerankãŒæœ‰åŠ¹ã‹)',\n",
    "        'RERANK_TOP_N (Rerankå¾Œä»¶æ•°)',\n",
    "        'CHAT_MODEL (ä½¿ç”¨LLMãƒ¢ãƒ‡ãƒ«)',\n",
    "        'MAX_TOKENS (æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°)',\n",
    "        'TEMPERATURE (æ¸©åº¦)',\n",
    "        'TOP_P (Nucleus sampling)',\n",
    "        'FREQUENCY_PENALTY (é »åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£)',\n",
    "        'TOP_K_SAMPLING (Top-K sampling)',\n",
    "        'ANSWER_PROMPT (å›ç­”ç”Ÿæˆæ™‚ã®æŒ‡ç¤ºæ–‡)',\n",
    "        'embedding_model',\n",
    "        'rerank_model',\n",
    "        'å®Ÿè¡Œæ—¥æ™‚',\n",
    "        'FAQä»¶æ•°',\n",
    "        'æˆåŠŸä»¶æ•°',\n",
    "        'å¤±æ•—ä»¶æ•°',\n",
    "        'å…¨ä½“å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰',\n",
    "        'å¹³å‡å‡¦ç†æ™‚é–“/ä»¶ï¼ˆç§’ï¼‰',\n",
    "        'ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å¹³å‡æ™‚é–“ï¼ˆç§’ï¼‰',\n",
    "        'Rerankå¹³å‡æ™‚é–“ï¼ˆç§’ï¼‰',\n",
    "        'å›ç­”ç”Ÿæˆå¹³å‡æ™‚é–“ï¼ˆç§’ï¼‰'\n",
    "    ],\n",
    "    'è¨­å®šå€¤': [\n",
    "        TOP_K,\n",
    "        RERANK_ENABLED,\n",
    "        RERANK_TOP_N,\n",
    "        CHAT_MODEL,\n",
    "        MAX_TOKENS,\n",
    "        TEMPERATURE,\n",
    "        TOP_P,\n",
    "        FREQUENCY_PENALTY,\n",
    "        TOP_K_SAMPLING,\n",
    "        ANSWER_PROMPT,\n",
    "        embedding_model,\n",
    "        'hotchpotch/japanese-reranker-base-v2',\n",
    "        datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        batch_result.total_questions,\n",
    "        batch_result.successful,\n",
    "        batch_result.failed,\n",
    "        f\"{batch_result.elapsed_time:.2f}\",\n",
    "        f\"{batch_result.elapsed_time/batch_result.total_questions:.2f}\",\n",
    "        f\"{results_df['vector_search_time'].mean():.2f}\",\n",
    "        f\"{results_df['rerank_time'].mean():.2f}\",\n",
    "        f\"{results_df['generation_time'].mean():.2f}\"\n",
    "    ]\n",
    "}\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "\n",
    "\n",
    "############################\n",
    "#  Object Storageã¸ä¿å­˜  # \n",
    "############################\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_filename = f\"rag_result_{timestamp}.xlsx\"\n",
    "\n",
    "# Object Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰\n",
    "print(f\"\\nğŸ“¤ Saving results to Object Storage...\")\n",
    "excel_handler.save_results(\n",
    "    results_df=results_df,\n",
    "    filename=output_filename,\n",
    "    metadata_df=metadata_df\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Results saved successfully\")\n",
    "print(f\"  - Bucket: {bucket_name}\")\n",
    "print(f\"  - Filename: {output_filename}\")\n",
    "print(f\"  - Rows: {len(results_df)}\")\n",
    "print(f\"  - Sheets: Results, Settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RAGASè©•ä¾¡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "\n",
    "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã®å“è³ªã‚’RAGASãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§è©•ä¾¡ã—ã¾ã™ã€‚\n",
    "\n",
    "**è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:\n",
    "- **Answer Correctness**: å›ç­”ã®æ­£ç¢ºæ€§ï¼ˆLLMåˆ¤å®š + åŸ‹ã‚è¾¼ã¿é¡ä¼¼åº¦ï¼‰\n",
    "- **Context Recall**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒæ­£è§£æƒ…å ±ã‚’ã©ã‚Œã ã‘ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã‹\n",
    "\n",
    "**æ³¨æ„**: è©•ä¾¡ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼ˆLLMå‘¼ã³å‡ºã—ãŒå¿…è¦ãªãŸã‚ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "############################\n#  RAGASè©•ä¾¡ã®å®Ÿè¡Œ  #\n############################\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Starting RAGAS Evaluation...\")\nprint(\"=\"*60 + \"\\n\")\n\n# RAGASè©•ä¾¡ç”¨ã®LLMãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ cohere.command-a-03-2025ï¼‰\nRAGAS_LLM_MODEL = \"cohere.command-a-03-2025\"\n\n# RagasEvaluatorã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\nevaluator = RagasEvaluator(\n    oci_config=oci_config,\n    compartment_id=compartment_id,\n    service_endpoint=genai_config['endpoint'],\n    embedding_model=embedding_model,\n    llm_model=RAGAS_LLM_MODEL,\n    genai_client=genai_client,\n    batch_size=3,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆå°ã•ãã™ã‚‹ã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å›é¿ã—ã‚„ã™ã„ï¼‰\n    max_retries=3,\n    retry_wait=30\n)\n\nprint(f\"âœ“ RagasEvaluator initialized\")\nprint(f\"  - LLM Model: {RAGAS_LLM_MODEL}\")\nprint(f\"  - Embedding Model: {embedding_model}\")\nprint(f\"  - Batch Size: 3\")\n\n# results_dfã‹ã‚‰è©•ä¾¡ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n# å¤±æ•—ã—ãŸè³ªå•ã¯é™¤å¤–\neval_df = results_df[results_df['status'] == 'success'].copy()\n\nif len(eval_df) == 0:\n    print(\"\\nâš  Warning: No successful results to evaluate\")\nelse:\n    # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n    questions = eval_df['question'].tolist()\n    answers = eval_df['answer'].tolist()\n    ground_truths = eval_df['ground_truth'].tolist()\n    \n    # contextsã®æº–å‚™ï¼ˆcontextsåˆ—ã‹ã‚‰è©•ä¾¡ç”¨ã®ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼‰\n    # contextsãŒæ–‡å­—åˆ—ã®å ´åˆã¯ãƒªã‚¹ãƒˆã«å¤‰æ›\n    contexts = []\n    for chunks in eval_df['contexts'].tolist():\n        if isinstance(chunks, list):\n            contexts.append(chunks)\n        elif isinstance(chunks, str):\n            # æ–‡å­—åˆ—ã®å ´åˆã¯ãƒªã‚¹ãƒˆã«å¤‰æ›\n            contexts.append([chunks])\n        else:\n            # ç©ºã®å ´åˆã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼\n            contexts.append([\"\"])\n    \n    print(f\"\\nEvaluating {len(eval_df)} questions...\")\n    print(f\"  Questions: {len(questions)}\")\n    print(f\"  Answers: {len(answers)}\")\n    print(f\"  Contexts: {len(contexts)}\")\n    print(f\"  Ground Truths: {len(ground_truths)}\")\n    \n    # RAGASè©•ä¾¡ã‚’å®Ÿè¡Œ\n    try:\n        eval_result = evaluator.evaluate(\n            questions=questions,\n            answers=answers,\n            contexts=contexts,\n            ground_truths=ground_truths\n        )\n        \n        # è©•ä¾¡çµæœã‚’DataFrameã«è¿½åŠ \n        eval_df['answer_correctness'] = eval_result.answer_correctness\n        eval_df['context_recall'] = eval_result.context_recall\n        \n        # çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º\n        print(\"\\n\" + \"=\"*60)\n        print(\"RAGAS Evaluation Complete\")\n        print(\"=\"*60)\n        print(f\"\\nğŸ“Š Evaluation Metrics Summary:\")\n        print(f\"  Answer Correctness (å¹³å‡): {eval_df['answer_correctness'].mean():.4f}\")\n        print(f\"  Answer Correctness (æœ€å°): {eval_df['answer_correctness'].min():.4f}\")\n        print(f\"  Answer Correctness (æœ€å¤§): {eval_df['answer_correctness'].max():.4f}\")\n        print(f\"  Context Recall (å¹³å‡): {eval_df['context_recall'].mean():.4f}\")\n        print(f\"  Context Recall (æœ€å°): {eval_df['context_recall'].min():.4f}\")\n        print(f\"  Context Recall (æœ€å¤§): {eval_df['context_recall'].max():.4f}\")\n        \n        # è©•ä¾¡çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼\n        print(f\"\\nğŸ“‹ Evaluation Results Preview:\")\n        print(eval_df[['id', 'question', 'answer_correctness', 'context_recall']].head())\n        \n        # å…ƒã®results_dfã«è©•ä¾¡çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆå¤±æ•—ã—ãŸè³ªå•ã¯è©•ä¾¡ã‚¹ã‚³ã‚¢ãŒNaNã«ãªã‚‹ï¼‰\n        results_df = results_df.merge(\n            eval_df[['id', 'answer_correctness', 'context_recall']],\n            on='id',\n            how='left'\n        )\n        \n        ############################\n        #  Object Storageã¸å†ä¿å­˜  #\n        ############################\n        \n        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆè©•ä¾¡çµæœä»˜ãï¼‰\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        output_filename_with_eval = f\"rag_result_with_ragas_{timestamp}.xlsx\"\n        \n        # Object Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè©•ä¾¡çµæœä»˜ãï¼‰\n        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯å›ç­”ç”Ÿæˆã‚»ãƒ«ã§ä½œæˆã—ãŸã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ç”¨\n        print(f\"\\nğŸ“¤ Saving results with RAGAS evaluation to Object Storage...\")\n        excel_handler.save_results(\n            results_df=results_df,\n            filename=output_filename_with_eval,\n            metadata_df=metadata_df\n        )\n        \n        print(f\"\\nâœ“ Results with RAGAS evaluation saved successfully\")\n        print(f\"  - Bucket: {bucket_name}\")\n        print(f\"  - Filename: {output_filename_with_eval}\")\n        print(f\"  - Rows: {len(results_df)}\")\n        print(f\"  - Sheets: Results, Settings\")\n        print(f\"  - New Columns: answer_correctness, context_recall\")\n        \n    except Exception as e:\n        print(f\"\\nâŒ RAGAS evaluation failed: {str(e)}\")\n        print(\"  You can continue without evaluation results.\")\n        import traceback\n        traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. DBæ¥ç¶šã®ã‚¯ãƒ­ãƒ¼ã‚º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "#  å¾Œå‡¦ç†  # \n",
    "################\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’æ˜ç¤ºçš„ã«ã‚¯ãƒ­ãƒ¼ã‚º\n",
    "connection.close()\n",
    "print(\"âœ“ Database connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}