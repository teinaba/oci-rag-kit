{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a509e88-04a2-48f4-b6d8-f173c54e0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "########  å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¾¤ ##########\n",
    "####################################\n",
    "# æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from io import BytesIO\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Oracle Database\n",
    "import oracledb\n",
    "\n",
    "# OCI\n",
    "import oci\n",
    "\n",
    "# LangChain\n",
    "from langchain_community.embeddings import OCIGenAIEmbeddings\n",
    "from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n",
    "\n",
    "# OCI GenAI Chat Model\n",
    "from oci.generative_ai_inference.models import (\n",
    "    CohereChatRequest,\n",
    "    GenericChatRequest,\n",
    "    OnDemandServingMode,\n",
    "    ChatDetails,\n",
    "    UserMessage,\n",
    "    TextContent\n",
    ")\n",
    "\n",
    "# Reranker\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "# RAGAS\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, AnswerCorrectness, ContextPrecision, ContextRecall\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "# è¨­å®šèª­ã¿è¾¼ã¿ï¼ˆconfig_loader.pyã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ï¼‰\n",
    "from config_loader import (\n",
    "    load_config,\n",
    "    get_db_connection_params,\n",
    "    get_oci_config,\n",
    "    get_genai_config,\n",
    "    get_object_storage_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "#  å®šæ•°åŠã³å¤‰æ•°  # \n",
    "################\n",
    "\n",
    "# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿\n",
    "load_config()\n",
    "\n",
    "# è¡¨å\n",
    "source_documents_table = \"source_documents\"\n",
    "chunks_table = \"chunks\"\n",
    "\n",
    "# Oracle Databaseæ¥ç¶š\n",
    "db_params = get_db_connection_params()\n",
    "connection = oracledb.connect(**db_params)\n",
    "print(connection)\n",
    "\n",
    "# OCIè¨­å®š\n",
    "config = get_oci_config()\n",
    "genai_config = get_genai_config()\n",
    "os_config = get_object_storage_config()\n",
    "\n",
    "# è¨­å®šå€¤ã‚’å¤‰æ•°ã«å±•é–‹\n",
    "region = config.get('region', 'ap-osaka-1')\n",
    "compartment_id = genai_config['compartment_id']\n",
    "\n",
    "# FAQãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®Object Storage\n",
    "bucket_name = \"faq\"\n",
    "object_name = \"faq.xlsx\"\n",
    "\n",
    "# LLM\n",
    "embedding_model = \"cohere.embed-v4.0\"\n",
    "chat_model = \"cohere.command-a-03-2025\"\n",
    "rerank_model = \"hotchpotch/japanese-reranker-base-v2\"\n",
    "service_endpoint = f\"https://inference.generativeai.{region}.oci.oraclecloud.com\"\n",
    "\n",
    "print(f\"\\nâœ“ è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "print(f\"  - Region: {region}\")\n",
    "print(f\"  - Embedding Model: {embedding_model}\")\n",
    "print(f\"  - Chat Model: {chat_model}\")\n",
    "print(f\"  - Rerank Model: {rerank_model}\")\n",
    "print(f\"  - FAQ Bucket: {bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Object Storageã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾— #\n",
    "######################################\n",
    "def load_excel_from_object_storage(config, bucket_name, object_name=\"faq.xlsx\", sheet_name=0):\n",
    "    \"\"\"\n",
    "    OCI Object Storageã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€DataFrameã‚’è¿”ã™\n",
    "    \n",
    "    Args:\n",
    "        config (dict): OCIè¨­å®šæƒ…å ±\n",
    "        bucket_name (str): Object Storageã®ãƒã‚±ãƒƒãƒˆå\n",
    "        object_name (str): Object Storageã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: \"faq.xlsx\"ï¼‰\n",
    "        sheet_name (str or int): èª­ã¿è¾¼ã‚€ã‚·ãƒ¼ãƒˆåã¾ãŸã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        df (DataFrame): ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€DataFrame\n",
    "    \"\"\"\n",
    "    object_storage_client = oci.object_storage.ObjectStorageClient(config)    \n",
    "    namespace = object_storage_client.get_namespace().data #type: ignore\n",
    "    \n",
    "    # Object Storageã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—\n",
    "    get_object_response = object_storage_client.get_object(namespace, bucket_name, object_name)\n",
    "    \n",
    "    # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’Excelã¨ã—ã¦èª­ã¿è¾¼ã¿\n",
    "    excel_data = BytesIO(get_object_response.data.content) #type: ignore\n",
    "    df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "    \n",
    "    # å¿…è¦ãªåˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªï¼ˆFAQãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Resultsã‚·ãƒ¼ãƒˆã®å ´åˆã®ã¿ï¼‰\n",
    "    if sheet_name == 0 or sheet_name == 'Results':\n",
    "        required_columns = ['id', 'question', 'ground_truth', 'filter']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"å¿…è¦ãªåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}\")\n",
    "    \n",
    "    print(\"\\nâœ“ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d19ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã‚’Excelã¨ã—ã¦Object Storageã«ä¿å­˜ #\n",
    "##############################################\n",
    "\n",
    "def save_to_object_storage(df, config, bucket_name, output_filename, metadata_df=None):\n",
    "    \"\"\"\n",
    "    DataFrameã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦OCI Object Storageã«ä¿å­˜\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): ä¿å­˜ã™ã‚‹DataFrame\n",
    "        config (dict): OCIè¨­å®šæƒ…å ±\n",
    "        bucket_name (str): Object Storageã®ãƒã‚±ãƒƒãƒˆå\n",
    "        output_filename (str): ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: \"rag_results.xlsx\"ï¼‰\n",
    "        metadata_df (DataFrame, optional): ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ã®DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        str: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®URLæƒ…å ±\n",
    "    \"\"\"\n",
    "    excel_buffer = BytesIO()\n",
    "    \n",
    "    # ExcelWriterã§è¤‡æ•°ã‚·ãƒ¼ãƒˆå¯¾å¿œ\n",
    "    with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        if metadata_df is not None:\n",
    "            metadata_df.to_excel(writer, sheet_name='Settings', index=False)\n",
    "    \n",
    "    excel_buffer.seek(0)\n",
    "    \n",
    "    # Object Storageã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ\n",
    "    object_storage_client = oci.object_storage.ObjectStorageClient(config)\n",
    "    namespace = object_storage_client.get_namespace().data #type: ignore\n",
    "    \n",
    "    try:\n",
    "        # Object Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "        object_storage_client.put_object(\n",
    "            namespace_name=namespace,\n",
    "            bucket_name=bucket_name,\n",
    "            object_name=output_filename,\n",
    "            put_object_body=excel_buffer.getvalue()\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Object Storageã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸ\")\n",
    "        print(f\"  ãƒã‚±ãƒƒãƒˆ: {bucket_name}\")\n",
    "        print(f\"  ãƒ•ã‚¡ã‚¤ãƒ«å: {output_filename}\")\n",
    "        print(f\"  è¡Œæ•°: {len(df)}\")\n",
    "        \n",
    "        return f\"Successfully uploaded to {bucket_name}/{output_filename}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Embeddingã™ã‚‹ #\n",
    "################\n",
    "\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=service_endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "\n",
    "embeddings = OCIGenAIEmbeddings(\n",
    "  model_id         = embedding_model,\n",
    "  service_endpoint = service_endpoint,\n",
    "  truncate         = \"NONE\",\n",
    "  compartment_id   = compartment_id,\n",
    "  auth_type        = \"API_KEY\",\n",
    "  client=generative_ai_inference_client\n",
    ")\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"\n",
    "    å˜ä¸€ã®æ–‡å­—åˆ—ã‚’Embeddingã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "    \n",
    "    Args:\n",
    "        text (str): Embeddingå¯¾è±¡ã®æ–‡å­—åˆ—\n",
    "        \n",
    "    Returns:\n",
    "        embedding (str_vector): Embeddingãƒ™ã‚¯ãƒˆãƒ«æ–‡å­—åˆ—ï¼ˆæµ®å‹•å°æ•°ç‚¹æ•°ã®ãƒªã‚¹ãƒˆæ–‡å­—åˆ—ï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    # Embeddingã®å®Ÿè¡Œ (pythonã®ãƒªã‚¹ãƒˆå½¢å¼): [0.1, 0.2, 0.3,...]\n",
    "    embedding_ = embeddings.embed_query(text)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ãƒ­ãƒ¼ãƒ‰ç”¨ã«å‹å¤‰æ›: [0.1, 0.2, 0.3,...] â†’ \"[0.1, 0.2, 0.3,...]\"\n",
    "    embedding = str(embedding_)\n",
    "        \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ #\n",
    "##############\n",
    "\n",
    "def vector_search(\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    filtering: str | None = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆãƒãƒ£ãƒ³ã‚¯å¯¾å¿œç‰ˆï¼‰\n",
    "    \n",
    "    Args:\n",
    "        query (str): æ¤œç´¢ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—\n",
    "        top_k (int, optional): è¿”å´ã™ã‚‹ä¸Šä½çµæœæ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯5\n",
    "        filtering (str, optional): ã‚½ãƒ¼ã‚¹ç¨®åˆ¥ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€‚Noneã®å ´åˆã¯å…¨ä»¶ã‚’å¯¾è±¡ã¨ã™ã‚‹\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    # 1. ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n",
    "    query_vector = embed_text(query)\n",
    "    \n",
    "    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š\n",
    "    connection = oracledb.connect(**db_params)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    try:\n",
    "        # 3. ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢\n",
    "        if filtering:\n",
    "            sql = f\"\"\"\n",
    "            SELECT \n",
    "                c.chunk_id,\n",
    "                c.document_id,\n",
    "                s.filename,\n",
    "                c.chunk_text,\n",
    "                VECTOR_DISTANCE(c.embedding, :query_vector, COSINE) as distance\n",
    "            FROM {chunks_table} c\n",
    "            JOIN {source_documents_table} s ON c.document_id = s.document_id\n",
    "            WHERE s.filtering = :filtering\n",
    "            ORDER BY VECTOR_DISTANCE(c.embedding, :query_vector, COSINE)\n",
    "            FETCH FIRST :top_k ROWS ONLY\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(sql, {\n",
    "                'query_vector': query_vector,\n",
    "                'filtering': filtering,\n",
    "                'top_k': top_k\n",
    "            })\n",
    "        else:\n",
    "            sql = f\"\"\"\n",
    "            SELECT \n",
    "                c.chunk_id,\n",
    "                c.document_id,\n",
    "                s.filename,\n",
    "                c.chunk_text,\n",
    "                VECTOR_DISTANCE(c.embedding, :query_vector, COSINE) as distance\n",
    "            FROM {chunks_table} c\n",
    "            JOIN {source_documents_table} s ON c.document_id = s.document_id\n",
    "            ORDER BY VECTOR_DISTANCE(c.embedding, :query_vector, COSINE)\n",
    "            FETCH FIRST :top_k ROWS ONLY\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(sql, {\n",
    "                'query_vector': query_vector,\n",
    "                'top_k': top_k\n",
    "            })\n",
    "        \n",
    "        # 4. çµæœã‚’å–å¾—\n",
    "        results = []\n",
    "        for row in cursor:\n",
    "            chunk_text_clob = row[3]\n",
    "            chunk_text_content = chunk_text_clob.read() if chunk_text_clob is not None else \"\"\n",
    "            \n",
    "            results.append({\n",
    "                'chunk_id': row[0],\n",
    "                'document_id': row[1],\n",
    "                'filename': row[2],\n",
    "                'chunk_text': chunk_text_content,\n",
    "                'distance': float(row[4])\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Rerankerãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ– (v2) #\n",
    "####################################\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Rerankerãƒ¢ãƒ‡ãƒ« (japanese-reranker-base-v2) ã‚’åˆæœŸåŒ–ä¸­...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•æ¤œå‡º\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif hasattr(torch, \"mps\") and torch.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "device = detect_device()\n",
    "print(f\"âœ“ æ¤œå‡ºãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "\n",
    "print(\"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "reranker_model = CrossEncoder(\n",
    "    'hotchpotch/japanese-reranker-base-v2',\n",
    "    max_length=512,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "if device in [\"cuda\", \"mps\"]:\n",
    "    print(\"âœ“ halfç²¾åº¦ã«å¤‰æ›ä¸­...\")\n",
    "    reranker_model.model.half()\n",
    "\n",
    "print(f\"âœ“ Rerankerãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº†\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Rerank #\n",
    "##########\n",
    "\n",
    "def rerank_chunks(query: str, chunks: List[Dict], top_n: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    hotchpotch/japanese-reranker-base-v2ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ£ãƒ³ã‚¯ã‚’å†ãƒ©ãƒ³ã‚¯ä»˜ã‘\n",
    "    \n",
    "    Args:\n",
    "        query (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—\n",
    "        chunks (List[Dict]): Vector Searchã§å–å¾—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ\n",
    "        top_n (int): æœ€çµ‚çš„ã«è¿”ã™ãƒãƒ£ãƒ³ã‚¯æ•°\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: Rerankã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆï¼ˆrerank_scoreã‚’å«ã‚€ï¼‰\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    # ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "    documents = [chunk['chunk_text'] for chunk in chunks]\n",
    "    \n",
    "    # ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒšã‚¢ã‚’ä½œæˆ\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    \n",
    "    try:\n",
    "        # Rerankerã§ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "        scores = reranker_model.predict(\n",
    "            pairs, \n",
    "            show_progress_bar=False,\n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        # å…ƒã®ãƒãƒ£ãƒ³ã‚¯ã«rerankã‚¹ã‚³ã‚¢ã‚’è¿½åŠ \n",
    "        for chunk, score in zip(chunks, scores):\n",
    "            chunk['rerank_score'] = float(score)\n",
    "        \n",
    "        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰\n",
    "        chunks.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        # ä¸Šä½top_nä»¶ã‚’è¿”ã™\n",
    "        return chunks[:top_n]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš  Rerankã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(f\"âš  Vector Searchã®çµæœä¸Šä½{top_n}ä»¶ã‚’ãã®ã¾ã¾è¿”ã—ã¾ã™\")\n",
    "        # Fallback: distanceã§ã‚½ãƒ¼ãƒˆï¼ˆå°ã•ã„æ–¹ãŒé¡ä¼¼ï¼‰\n",
    "        return sorted(chunks, key=lambda x: x.get('distance', float('inf')))[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72302485",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# å›ç­”ç”Ÿæˆ #\n",
    "###########\n",
    "\n",
    "def generate_answer(\n",
    "    query: str,\n",
    "    contexts: str,\n",
    "    chat_model: str = \"cohere.command-a-03-2025\",\n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 0.3,\n",
    "    top_p: float = 0.75,\n",
    "    frequency_penalty: float = 0.0,\n",
    "    top_k: int = 0,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: int = 60,\n",
    "    answer_prompt: str = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    æ¤œç´¢çµæœã‚’å…ƒã«å›ç­”ã‚’ç”Ÿæˆï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰\n",
    "\n",
    "    Args:\n",
    "        query (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•\n",
    "        contexts (str): å‚è€ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆçµåˆæ¸ˆã¿ï¼‰\n",
    "        chat_model (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«IDã€‚ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ãŒä½¿ãˆã¾ã™:\n",
    "            ã€Cohereã€‘\n",
    "            - \"cohere.command-a-03-2025\" (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)\n",
    "            - \"cohere.command-r-plus-08-2024\"\n",
    "            ã€Meta Llamaã€‘\n",
    "            - \"meta.llama-3.3-70b-instruct\"\n",
    "            ã€xAI Grokã€‘\n",
    "            - \"xai.grok-4-fast-non-reasoning\" (RAGæ¨å¥¨)\n",
    "            - \"xai.grok-4-fast-reasoning\" (è¤‡é›‘ãªæ¨è«–ç”¨)\n",
    "            - \"xai.grok-4\"\n",
    "            ã€Google Geminiã€‘\n",
    "            - \"google.gemini-2.5-pro\"\n",
    "            - \"google.gemini-2.5-flash\"\n",
    "            - \"google.gemini-2.5-flash-lite\"\n",
    "            ã€OpenAI GPT-OSSã€‘\n",
    "            - \"openai.gpt-oss-20b\"\n",
    "            - \"openai.gpt-oss-120b\"\n",
    "        max_tokens (int): æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "        temperature (float): æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0-1ã€å‰µé€ æ€§ã®åˆ¶å¾¡ï¼‰\n",
    "        top_p (float): Nucleus samplingã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0-1ï¼‰\n",
    "        frequency_penalty (float): é »åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆCohereç”¨ã€0-1ï¼‰\n",
    "        top_k (int): Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆCohereç”¨ã€0ã§ç„¡åŠ¹ï¼‰\n",
    "        max_retries (int): HTTP 429ç™ºç”Ÿæ™‚ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯3\n",
    "        retry_delay (int): ãƒªãƒˆãƒ©ã‚¤é–“éš”ã®åˆæœŸå€¤ï¼ˆç§’ï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯60ç§’\n",
    "        answer_prompt (str): å›ç­”ç”Ÿæˆæ™‚ã®æŒ‡ç¤ºæ–‡\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: ç”Ÿæˆçµæœã®è¾æ›¸ã€‚ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤:\n",
    "            - answer (str): ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "            - model_used (str): ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ID\n",
    "    \"\"\"\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ\n",
    "    prompt = f\"\"\"ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚è€ƒã«ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ã€å‚è€ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€‘\n",
    "{contexts}\n",
    "\n",
    "ã€è³ªå•ã€‘\n",
    "{query}\n",
    "\n",
    "ã€å›ç­”ã€‘\n",
    "{answer_prompt}\n",
    "\"\"\"\n",
    "\n",
    "    # Cohereãƒ¢ãƒ‡ãƒ«ç”¨ï¼ˆCohereChatRequestï¼‰\n",
    "    if \"cohere\" in chat_model.lower():\n",
    "        chat_request = CohereChatRequest(\n",
    "            message=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        chat_detail = ChatDetails(\n",
    "            serving_mode=OnDemandServingMode(model_id=chat_model),\n",
    "            compartment_id=compartment_id,\n",
    "            chat_request=chat_request\n",
    "        )\n",
    "\n",
    "        # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                # å›ç­”ç”Ÿæˆå®Ÿè¡Œ\n",
    "                response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰å›ç­”ã‚’å–å¾—\n",
    "                answer = response.data.chat_response.text #type: ignore\n",
    "\n",
    "                return {\n",
    "                    'answer': answer,\n",
    "                    'model_used': chat_model\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                # HTTP 429ã‚¨ãƒ©ãƒ¼ï¼ˆRate Limitï¼‰ã®å ´åˆ\n",
    "                if hasattr(e, 'status') and e.status == 429:\n",
    "                    if attempt < max_retries:\n",
    "                        # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤é–“éš”ã‚’å»¶é•·\n",
    "                        wait_time = retry_delay * (2 ** attempt)\n",
    "                        print(f\"HTTP 429ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã€‚{wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... (è©¦è¡Œ {attempt + 1}/{max_retries})\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ãŸå ´åˆ\n",
    "                        raise Exception(f\"æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°({max_retries})ã«é”ã—ã¾ã—ãŸã€‚HTTP 429ã‚¨ãƒ©ãƒ¼ãŒç¶™ç¶šã—ã¦ã„ã¾ã™ã€‚\") from e\n",
    "                else:\n",
    "                    # 429ä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«å†é€å‡º\n",
    "                    raise\n",
    "\n",
    "    # ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«ç”¨ï¼ˆGenericChatRequestï¼‰\n",
    "    # Llama, Grok, Gemini, OpenAI GPT-OSS ãªã©\n",
    "    else:\n",
    "        # UserMessageã¨TextContentã‚’ä½¿ã£ãŸæ­£ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "        messages = [\n",
    "            UserMessage(content=[TextContent(text=prompt)])\n",
    "        ]\n",
    "\n",
    "        chat_request = GenericChatRequest(\n",
    "            api_format=\"GENERIC\",\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "\n",
    "        chat_detail = ChatDetails(\n",
    "            serving_mode=OnDemandServingMode(model_id=chat_model),\n",
    "            compartment_id=compartment_id,\n",
    "            chat_request=chat_request\n",
    "        )\n",
    "\n",
    "        # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                # å›ç­”ç”Ÿæˆå®Ÿè¡Œ\n",
    "                response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰å›ç­”ã‚’å–å¾—\n",
    "                # GenericChatRequestã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "                answer = response.data.chat_response.choices[0].message.content[0].text #type: ignore\n",
    "\n",
    "                return {\n",
    "                    'answer': answer,\n",
    "                    'model_used': chat_model\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                # HTTP 429ã‚¨ãƒ©ãƒ¼ï¼ˆRate Limitï¼‰ã®å ´åˆ\n",
    "                if hasattr(e, 'status') and e.status == 429:\n",
    "                    if attempt < max_retries:\n",
    "                        # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤é–“éš”ã‚’å»¶é•·\n",
    "                        wait_time = retry_delay * (2 ** attempt)\n",
    "                        print(f\"HTTP 429ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã€‚{wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... (è©¦è¡Œ {attempt + 1}/{max_retries})\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ãŸå ´åˆ\n",
    "                        raise Exception(f\"æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°({max_retries})ã«é”ã—ã¾ã—ãŸã€‚HTTP 429ã‚¨ãƒ©ãƒ¼ãŒç¶™ç¶šã—ã¦ã„ã¾ã™ã€‚\") from e\n",
    "                else:\n",
    "                    # 429ä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«å†é€å‡º\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532fad4",
   "metadata": {},
   "outputs": [],
   "source": "################\n# RAGASè©•ä¾¡é–¢æ•° #\n################\n\ndef evaluate_with_ragas(question, answer, contexts, ground_truth):\n    \"\"\"\n    RAGã‚·ã‚¹ãƒ†ãƒ ã®å‡ºåŠ›ã‚’RAGASã§è©•ä¾¡\n    \n    Args:\n        question (List[str]): è³ªå•ã®ãƒªã‚¹ãƒˆ\n        answer (List[str]): RAGã‚·ã‚¹ãƒ†ãƒ ãŒç”Ÿæˆã—ãŸå›ç­”ã®ãƒªã‚¹ãƒˆ\n        contexts (List[List[str]]): å„è³ªå•ã«å¯¾ã—ã¦æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼‰\n        ground_truth (List[str]): æ­£è§£ã¨ãªã‚‹å›ç­”ã®ãƒªã‚¹ãƒˆ\n    \n    Returns:\n        result (dict): è©•ä¾¡çµæœ\n            - answer_correctness\n            - context_recall\n    \"\"\"\n    # RAGASè©•ä¾¡å°‚ç”¨ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·ç‰ˆï¼‰\n    generative_ai_client_for_ragas = oci.generative_ai_inference.GenerativeAiInferenceClient(\n        config=config,\n        service_endpoint=service_endpoint,\n        retry_strategy=oci.retry.NoneRetryStrategy(),\n        timeout=(30, 1200)  # æ¥ç¶š30ç§’ã€èª­ã¿å–ã‚Š1200ç§’ã«å»¶é•·\n    )\n    \n    # LLMã®æº–å‚™\n    llm4eval = ChatOCIGenAI(\n        model_id=\"cohere.command-a-03-2025\",\n        service_endpoint=service_endpoint,\n        compartment_id=compartment_id,\n        is_stream=False,\n        model_kwargs={\"temperature\": 0.0, \"max_tokens\": 4000},\n        auth_type=\"API_KEY\",\n        client=generative_ai_client_for_ragas)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·ç‰ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨\n\n    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n    embeddings4eval = OCIGenAIEmbeddings(\n        model_id=embedding_model,\n        service_endpoint=service_endpoint,\n        compartment_id=compartment_id,\n        truncate=\"END\",\n        auth_type=\"API_KEY\",\n        client=generative_ai_client_for_ragas)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·ç‰ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨\n\n    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n    ds = Dataset.from_dict(\n        {\n            \"question\": question,\n            \"answer\": answer,\n            \"contexts\": contexts,\n            \"ground_truth\": ground_truth,\n        }\n    )\n\n    # OCI Generative AI Cohere Chatç”¨ã®finished_parserã‚’å®Ÿè£…\n    def my_finished_parser(response: LLMResult) -> bool:\n        if (response.generations \n            and response.generations[0] \n            and response.generations[0][0].generation_info \n            and response.generations[0][0].generation_info.get('finish_reason') == 'COMPLETE'):\n            return True\n        return False\n\n    # è©•ä¾¡ç”¨LLMã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆï¼ˆã“ã“ã§ã€finished_parserã‚’è¨­å®šï¼‰\n    evaluator_llm = LangchainLLMWrapper(llm4eval, is_finished_parser=my_finished_parser)\n\n    # è©•ä¾¡ç”¨åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n    evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings4eval)\n\n    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆanswer_correctness ã¨ context_recall ã®ã¿ï¼‰\n    metrics = [\n        AnswerCorrectness(llm=evaluator_llm, embeddings=evaluator_embeddings),\n        ContextRecall(llm=evaluator_llm)\n    ]\n\n    # è©•ä¾¡ã®å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯æ˜ç¤ºçš„ã«åœæ­¢ï¼‰\n    result = evaluate(ds, metrics, raise_exceptions=True)\n\n    return result"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e38140-44a2-4167-8124-e825ab1dc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# ãƒ¡ã‚¤ãƒ³å‡¦ç†1: Excelã®è³ªå•ã¸ã®å›ç­”ç”Ÿæˆ #\n",
    "###################################\n",
    "\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "TOP_K = 10  # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ä¸Šä½ä½•ä»¶ã‚’è¿”å´ã™ã‚‹ã‹ã€‚Rerankç„¡åŠ¹æ™‚ã¯ã“ã®å€¤ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»¶æ•°\n",
    "\n",
    "# Rerankã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (Rerankã¯CPUå‡¦ç†æ€§èƒ½ã«ä¾ã£ã¦ã¯é•·æ™‚é–“è¦ã™ã‚‹ãŸã‚æ³¨æ„)\n",
    "RERANK_ENABLED = True # True or False\n",
    "RERANK_TOP_N = 5  # Rerankå¾Œã«ä¸Šä½ä½•ä»¶ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ã‹\n",
    "\n",
    "# LLMé¸æŠãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "# ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼š\n",
    "# ã€Cohereã€‘ \"cohere.command-a-03-2025\", \"cohere.command-r-plus-08-2024\"\n",
    "# ã€Meta Llamaã€‘ \"meta.llama-3.3-70b-instruct\"\n",
    "# ã€xAI Grokã€‘ \"xai.grok-4-fast-non-reasoning\", \"xai.grok-4-fast-reasoning\", \"xai.grok-4\"\n",
    "# ã€Google Geminiã€‘ \"google.gemini-2.5-pro\", \"google.gemini-2.5-flash\", \"google.gemini-2.5-flash-lite\"\n",
    "# ã€OpenAI GPT-OSSã€‘ \"openai.gpt-oss-20b\", \"openai.gpt-oss-120b\"\n",
    "CHAT_MODEL = \"cohere.command-a-03-2025\"  # ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š\n",
    "\n",
    "# LLMãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "MAX_TOKENS = 1000  # ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "TEMPERATURE = 0.3  # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0-1ã€å‰µé€ æ€§ã®åˆ¶å¾¡ã€‚0ã«è¿‘ã„ã»ã©æ±ºå®šçš„ã€1ã«è¿‘ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ï¼‰\n",
    "TOP_P = 0.75  # Nucleus samplingã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0-1ï¼‰\n",
    "FREQUENCY_PENALTY = 0.0  # é »åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆCohereç”¨ã€0-1ã€‚ç¹°ã‚Šè¿”ã—ã‚’æŠ‘åˆ¶ï¼‰\n",
    "TOP_K_SAMPLING = 0  # Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆCohereç”¨ã€0ã§ç„¡åŠ¹ï¼‰\n",
    "\n",
    "# å›ç­”ç”Ÿæˆæ™‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "ANSWER_PROMPT = \"\"\"\n",
    "å‚è€ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æƒ…å ±ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’ä¼ãˆã¦ãã ã•ã„ã€‚\n",
    "å›ç­”ã¯ç°¡æ½”ã«å¹³æ–‡ã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# FAQ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰\n",
    "faq_df = load_excel_from_object_storage(config, bucket_name, object_name)\n",
    "\n",
    "print(f\"FAQãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦RAGå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆå…¨{len(faq_df)}ä»¶ï¼‰\")\n",
    "print(f\"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {CHAT_MODEL}\\n\")\n",
    "\n",
    "# answer, contexts, ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ç”¨ã®åˆ—ã‚’æ–°è¦ä½œæˆ\n",
    "faq_df['answer'] = None\n",
    "faq_df['contexts'] = None\n",
    "faq_df['vector_search_time'] = 0.0\n",
    "faq_df['rerank_time'] = 0.0\n",
    "faq_df['generation_time'] = 0.0\n",
    "faq_df['total_time'] = 0.0\n",
    "\n",
    "# å…¨ä½“å‡¦ç†æ™‚é–“ã®è¨ˆæ¸¬é–‹å§‹\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# å„è³ªå•ã«å¯¾ã—ã¦å‡¦ç†\n",
    "for idx, row in faq_df.iterrows():\n",
    "    question = row['question']\n",
    "    filter_value = row['filter']\n",
    "    \n",
    "    if pd.notna(filter_value) and filter_value != \"\":\n",
    "        filtering = filter_value\n",
    "    else:\n",
    "        filtering = None\n",
    "    \n",
    "    print(f\"[{idx + 1}/{len(faq_df)}] å‡¦ç†ä¸­: {question[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆæ™‚é–“è¨ˆæ¸¬ï¼‰\n",
    "        vector_search_start = time.time()\n",
    "        candidates = vector_search(\n",
    "            query=question,\n",
    "            top_k=TOP_K,\n",
    "            filtering=filtering\n",
    "        )\n",
    "        vector_search_time = time.time() - vector_search_start\n",
    "        faq_df.at[idx, 'vector_search_time'] = vector_search_time\n",
    "\n",
    "        # 2. Rerankingã§çµã‚Šè¾¼ã¿ï¼ˆæ™‚é–“è¨ˆæ¸¬ï¼‰\n",
    "        rerank_start = time.time()\n",
    "        if RERANK_ENABLED:\n",
    "            search_results = rerank_chunks(question, candidates, top_n=RERANK_TOP_N)\n",
    "        else:\n",
    "            search_results = candidates[:RERANK_TOP_N]\n",
    "        rerank_time = time.time() - rerank_start\n",
    "        faq_df.at[idx, 'rerank_time'] = rerank_time\n",
    " \n",
    "        # 3. contextsã‚’ä½œæˆï¼ˆLLMã«æ¸¡ã™ç”¨ & DataFrameæ ¼ç´ç”¨ã§åŒä¸€ï¼‰\n",
    "        contexts = \"\\n\\n\".join([\n",
    "            f\"[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {i+1}: {result['filename']}]\\n{result['chunk_text']}\"\n",
    "            for i, result in enumerate(search_results)\n",
    "        ])\n",
    "        \n",
    "        # 4. LLMã§å›ç­”ç”Ÿæˆï¼ˆæ™‚é–“è¨ˆæ¸¬ï¼‰\n",
    "        generation_start = time.time()\n",
    "        result = generate_answer(\n",
    "            query=question,\n",
    "            contexts=contexts,\n",
    "            chat_model=CHAT_MODEL,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            frequency_penalty=FREQUENCY_PENALTY,\n",
    "            top_k=TOP_K_SAMPLING,\n",
    "            answer_prompt=ANSWER_PROMPT\n",
    "        )\n",
    "        generation_time = time.time() - generation_start\n",
    "        faq_df.at[idx, 'generation_time'] = generation_time\n",
    "        \n",
    "        # 5. åˆè¨ˆæ™‚é–“ã‚’è¨ˆç®—\n",
    "        total_time = vector_search_time + rerank_time + generation_time\n",
    "        faq_df.at[idx, 'total_time'] = total_time\n",
    "        \n",
    "        # 6. DataFrameã«çµæœã‚’æ ¼ç´\n",
    "        faq_df.at[idx, 'answer'] = result['answer']\n",
    "        faq_df.at[idx, 'contexts'] = contexts\n",
    "        \n",
    "        print(f\"  âœ“ å®Œäº† (æ¤œç´¢çµæœ: {len(search_results)}ä»¶, å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’)\")\n",
    "        print(f\"    - ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢: {vector_search_time:.2f}ç§’\")\n",
    "        print(f\"    - Rerank: {rerank_time:.2f}ç§’\")\n",
    "        print(f\"    - å›ç­”ç”Ÿæˆ: {generation_time:.2f}ç§’\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        faq_df.at[idx, 'answer'] = \"\"\n",
    "        faq_df.at[idx, 'contexts'] = \"\"\n",
    "        continue\n",
    "\n",
    "# å…¨ä½“å‡¦ç†æ™‚é–“ã®è¨ˆæ¸¬çµ‚äº†\n",
    "overall_end_time = time.time()\n",
    "overall_processing_time = overall_end_time - overall_start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ RAGå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"å…¨ä½“å‡¦ç†æ™‚é–“: {overall_processing_time:.2f}ç§’\")\n",
    "print(f\"å¹³å‡å‡¦ç†æ™‚é–“: {overall_processing_time/len(faq_df):.2f}ç§’/ä»¶\")\n",
    "print(f\"\\nã€å‡¦ç†æ™‚é–“ã®çµ±è¨ˆã€‘\")\n",
    "print(f\"  ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å¹³å‡: {faq_df['vector_search_time'].mean():.2f}ç§’\")\n",
    "print(f\"  Rerankå¹³å‡: {faq_df['rerank_time'].mean():.2f}ç§’\")\n",
    "print(f\"  å›ç­”ç”Ÿæˆå¹³å‡: {faq_df['generation_time'].mean():.2f}ç§’\")\n",
    "print(f\"  åˆè¨ˆå¹³å‡: {faq_df['total_time'].mean():.2f}ç§’\")\n",
    "\n",
    "# çµæœã‚’ç¢ºèª\n",
    "print(\"\\nå‡¦ç†çµæœ:\")\n",
    "print(faq_df[['id', 'question', 'ground_truth', 'filter', 'answer', 'total_time']].head())\n",
    "\n",
    "# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’å«ã‚€ï¼‰\n",
    "metadata = {\n",
    "    'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿': [\n",
    "        'TOP_K (ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ä»¶æ•°)',\n",
    "        'RERANK_ENABLED (RerankãŒæœ‰åŠ¹ã‹)',\n",
    "        'RERANK_TOP_N (Rerankå¾Œä»¶æ•°)',\n",
    "        'CHAT_MODEL (ä½¿ç”¨LLMãƒ¢ãƒ‡ãƒ«)',\n",
    "        'MAX_TOKENS (æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°)',\n",
    "        'TEMPERATURE (æ¸©åº¦)',\n",
    "        'TOP_P (Nucleus sampling)',\n",
    "        'FREQUENCY_PENALTY (é »åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£)',\n",
    "        'TOP_K_SAMPLING (Top-K sampling)',\n",
    "        'ANSWER_PROMPT (å›ç­”ç”Ÿæˆæ™‚ã®æŒ‡ç¤ºæ–‡)',\n",
    "        'embedding_model',\n",
    "        'rerank_model',\n",
    "        'service_endpoint',\n",
    "        'å®Ÿè¡Œæ—¥æ™‚',\n",
    "        'FAQä»¶æ•°',\n",
    "        'å…¨ä½“å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰',\n",
    "        'å¹³å‡å‡¦ç†æ™‚é–“/ä»¶ï¼ˆç§’ï¼‰',\n",
    "        'ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å¹³å‡æ™‚é–“ï¼ˆç§’ï¼‰',\n",
    "        'Rerankå¹³å‡æ™‚é–“ï¼ˆç§’ï¼‰',\n",
    "        'å›ç­”ç”Ÿæˆå¹³å‡æ™‚é–“ï¼ˆç§’ï¼‰'\n",
    "    ],\n",
    "    'è¨­å®šå€¤': [\n",
    "        TOP_K,\n",
    "        RERANK_ENABLED,\n",
    "        RERANK_TOP_N,\n",
    "        CHAT_MODEL,\n",
    "        MAX_TOKENS,\n",
    "        TEMPERATURE,\n",
    "        TOP_P,\n",
    "        FREQUENCY_PENALTY,\n",
    "        TOP_K_SAMPLING,\n",
    "        ANSWER_PROMPT,\n",
    "        embedding_model,\n",
    "        rerank_model,\n",
    "        service_endpoint,\n",
    "        datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        len(faq_df),\n",
    "        f\"{overall_processing_time:.2f}\",\n",
    "        f\"{overall_processing_time/len(faq_df):.2f}\",\n",
    "        f\"{faq_df['vector_search_time'].mean():.2f}\",\n",
    "        f\"{faq_df['rerank_time'].mean():.2f}\",\n",
    "        f\"{faq_df['generation_time'].mean():.2f}\"\n",
    "    ]\n",
    "}\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ã¿ï¼‰\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_filename = f\"rag_result_{timestamp}.xlsx\"\n",
    "\n",
    "# Object Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰\n",
    "save_to_object_storage(faq_df, config, bucket_name, output_filename, metadata_df=metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1500db",
   "metadata": {},
   "outputs": [],
   "source": "#############################\n# ãƒ¡ã‚¤ãƒ³å‡¦ç†2: RAGASè©•ä¾¡ã®å®Ÿè¡Œ #\n#############################\n\nprint(f\"\\n{'='*60}\")\nprint(f\"RAGASè©•ä¾¡å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™\")\nprint(f\"{'='*60}\\n\")\n\n# ãƒãƒƒãƒã‚µã‚¤ã‚ºã®è¨­å®šï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ã¨ã—ã¦3ä»¶ã«å‰Šæ¸›ï¼‰\nRAGAS_BATCH_SIZE = 3\n\n# ãƒªãƒˆãƒ©ã‚¤è¨­å®š\nMAX_RETRIES = 3  # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°\nRETRY_WAIT_TIME = 30  # ãƒªãƒˆãƒ©ã‚¤å‰ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰\n\n# è©•ä¾¡å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŒ‡å®š\ninput_filename = output_filename  # å€‹åˆ¥ã§å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹å ´åˆã¯ã€ã“ã“ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„\n\n# 1. Object Storageã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\nprint(f\"1. Object Storageã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\ndf_for_ragas = load_excel_from_object_storage(config, bucket_name, input_filename, sheet_name='Results')\n\nprint(f\"   âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(df_for_ragas)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿\")\nprint(f\"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {RAGAS_BATCH_SIZE}ä»¶ãšã¤å‡¦ç†\")\nprint(f\"   æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°: {MAX_RETRIES}å›\")\n\n# 2. RAGASè©•ä¾¡ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\nprint(f\"\\n2. RAGASè©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...\")\n\n# DataFrameã‹ã‚‰å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\nquestions = df_for_ragas['question'].tolist()\nanswers = df_for_ragas['answer'].tolist()\nground_truths = df_for_ragas['ground_truth'].tolist()\n\n# contextsã‚’ãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼ˆRAGASã®è¦æ±‚å½¢å¼ï¼‰\n# å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å€‹åˆ¥è¦ç´ ã«åˆ†å‰²ã™ã‚‹\ncontexts_list = []\nfor ctx in df_for_ragas['contexts'].tolist():\n    if pd.isna(ctx) or ctx == \"\":\n        contexts_list.append([\"\"])\n    else:\n        # \"[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\" ã§å§‹ã¾ã‚‹è¡Œã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²\n        lines = ctx.split('\\n')\n        individual_docs = []\n        current_doc = \"\"\n        \n        for line in lines:\n            if line.startswith('[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ'):\n                # æ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå§‹ã¾ã‚‹\n                if current_doc:\n                    individual_docs.append(current_doc.strip())\n                current_doc = line + '\\n'\n            else:\n                current_doc += line + '\\n'\n        \n        # æœ€å¾Œã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ \n        if current_doc:\n            individual_docs.append(current_doc.strip())\n        \n        if individual_docs:\n            contexts_list.append(individual_docs)\n        else:\n            # åˆ†å‰²ã§ããªã‹ã£ãŸå ´åˆã¯å…ƒã®ã¾ã¾\n            contexts_list.append([ctx])\n\nprint(f\"   âœ“ ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†\")\n\n# 3. RAGASè©•ä¾¡ã‚’ãƒãƒƒãƒå‡¦ç†ã§å®Ÿè¡Œ\nprint(f\"\\n3. RAGASè©•ä¾¡ã‚’å®Ÿè¡Œä¸­...\")\nprint(f\"   ï¼ˆã“ã®å‡¦ç†ã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰\")\n\ntotal_samples = len(questions)\nall_results = []\nbatch_times = []  # ãƒãƒƒãƒã”ã¨ã®å‡¦ç†æ™‚é–“ã‚’è¨˜éŒ²\nretry_stats = {'total_retries': 0, 'successful_retries': 0, 'failed_batches': 0}\n\noverall_start_time = time.time()\n\ntry:\n    for i in range(0, total_samples, RAGAS_BATCH_SIZE):\n        batch_start_time = time.time()\n        \n        batch_end = min(i + RAGAS_BATCH_SIZE, total_samples)\n        batch_num = i // RAGAS_BATCH_SIZE + 1\n        total_batches = (total_samples + RAGAS_BATCH_SIZE - 1) // RAGAS_BATCH_SIZE\n        \n        print(f\"\\n   ãƒãƒƒãƒ {batch_num}/{total_batches} è©•ä¾¡ä¸­ ({i+1}-{batch_end}/{total_samples}ä»¶)\")\n        \n        # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n        batch_questions = questions[i:batch_end]\n        batch_answers = answers[i:batch_end]\n        batch_contexts = contexts_list[i:batch_end]\n        batch_ground_truths = ground_truths[i:batch_end]\n        \n        # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ä»˜ããƒãƒƒãƒè©•ä¾¡å®Ÿè¡Œ\n        batch_success = False\n        batch_result = None\n        \n        for attempt in range(MAX_RETRIES + 1):\n            try:\n                if attempt > 0:\n                    print(f\"   ğŸ”„ ãƒªãƒˆãƒ©ã‚¤ {attempt}/{MAX_RETRIES}...\")\n                    retry_stats['total_retries'] += 1\n                    # ãƒªãƒˆãƒ©ã‚¤å‰ã«å¾…æ©Ÿ\n                    time.sleep(RETRY_WAIT_TIME)\n                \n                batch_result = evaluate_with_ragas(\n                    batch_questions,\n                    batch_answers,\n                    batch_contexts,\n                    batch_ground_truths\n                )\n                \n                # æˆåŠŸã—ãŸå ´åˆ\n                batch_success = True\n                if attempt > 0:\n                    retry_stats['successful_retries'] += 1\n                    print(f\"   âœ“ ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ\")\n                break\n                \n            except Exception as batch_error:\n                if attempt < MAX_RETRIES:\n                    print(f\"   âš  è©¦è¡Œ {attempt + 1} ã§ã‚¨ãƒ©ãƒ¼: {str(batch_error)[:100]}...\")\n                    continue\n                else:\n                    # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ãŸå ´åˆ\n                    print(f\"   âœ— ãƒãƒƒãƒ {batch_num} ã§æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«åˆ°é”\")\n                    print(f\"   æœ€çµ‚ã‚¨ãƒ©ãƒ¼: {str(batch_error)[:100]}...\")\n                    retry_stats['failed_batches'] += 1\n        \n        batch_time = time.time() - batch_start_time\n        batch_times.append(batch_time)\n        \n        # çµæœã‚’ä¿å­˜\n        if batch_success and batch_result is not None:\n            all_results.append(batch_result.to_pandas())\n            print(f\"   âœ“ ãƒãƒƒãƒ {batch_num} å®Œäº† (å‡¦ç†æ™‚é–“: {batch_time:.2f}ç§’)\")\n        else:\n            # å…¨ã¦ã®ãƒªãƒˆãƒ©ã‚¤ãŒå¤±æ•—ã—ãŸå ´åˆã¯ç©ºã®DataFrameã§åŸ‹ã‚ã‚‹\n            error_df = pd.DataFrame({\n                'answer_correctness': [None] * len(batch_questions),\n                'context_recall': [None] * len(batch_questions)\n            })\n            all_results.append(error_df)\n            print(f\"   âœ— ãƒãƒƒãƒ {batch_num} ã¯å¤±æ•—ã—ã¾ã—ãŸ (å‡¦ç†æ™‚é–“: {batch_time:.2f}ç§’)\")\n        \n        # Rate Limitå¯¾ç­–ï¼šãƒãƒƒãƒé–“ã§å¾…æ©Ÿï¼ˆæœ€çµ‚ãƒãƒƒãƒä»¥å¤–ï¼‰\n        if batch_end < total_samples:\n            print(f\"   æ¬¡ã®ãƒãƒƒãƒã¾ã§15ç§’å¾…æ©Ÿä¸­...\")\n            time.sleep(15)\n    \n    # çµæœã‚’çµ±åˆ\n    result_df = pd.concat(all_results, ignore_index=True)\n    \n    overall_time = time.time() - overall_start_time\n    \n    print(f\"\\n   âœ“ å…¨ãƒãƒƒãƒã®RAGASè©•ä¾¡å®Œäº†\")\n    print(f\"\\n{'='*60}\")\n    print(f\"RAGASè©•ä¾¡çµæœã‚µãƒãƒªãƒ¼\")\n    print(f\"{'='*60}\")\n    print(f\"ç·å‡¦ç†æ™‚é–“: {overall_time:.2f}ç§’\")\n    print(f\"å¹³å‡ãƒãƒƒãƒå‡¦ç†æ™‚é–“: {sum(batch_times)/len(batch_times):.2f}ç§’\")\n    print(f\"æœ€å¤§ãƒãƒƒãƒå‡¦ç†æ™‚é–“: {max(batch_times):.2f}ç§’\")\n    print(f\"æœ€å°ãƒãƒƒãƒå‡¦ç†æ™‚é–“: {min(batch_times):.2f}ç§’\")\n    \n    # ãƒªãƒˆãƒ©ã‚¤çµ±è¨ˆ\n    print(f\"\\nã€ãƒªãƒˆãƒ©ã‚¤çµ±è¨ˆã€‘\")\n    print(f\"  ç·ãƒªãƒˆãƒ©ã‚¤å›æ•°: {retry_stats['total_retries']}å›\")\n    print(f\"  ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ: {retry_stats['successful_retries']}å›\")\n    print(f\"  æœ€çµ‚çš„ã«å¤±æ•—ã—ãŸãƒãƒƒãƒ: {retry_stats['failed_batches']}å€‹\")\n    \nexcept Exception as e:\n    print(f\"\\n   âœ— RAGASè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}\")\n    raise\n\n# 4. è©•ä¾¡çµæœã‚’DataFrameã«è¿½è¨˜\nprint(f\"\\n4. è©•ä¾¡çµæœã‚’DataFrameã«è¿½è¨˜ä¸­...\")\n\n# å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’DataFrameã«è¿½åŠ \ndf_for_ragas['answer_correctness'] = result_df['answer_correctness']\ndf_for_ragas['context_recall'] = result_df['context_recall']\n\nprint(f\"   âœ“ è¿½è¨˜å®Œäº†\")\n\n# 5. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆã‚‚èª­ã¿è¾¼ã‚“ã§ä¿æŒ\nprint(f\"\\n5. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\nmetadata_df = load_excel_from_object_storage(config, bucket_name, input_filename, sheet_name='Settings')\nprint(f\"   âœ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†\")\n\n# 6. æ›´æ–°ã—ãŸExcelã‚’Object Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\nprint(f\"\\n6. è©•ä¾¡çµæœã‚’Object Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n\n# ãƒ•ã‚¡ã‚¤ãƒ«åã« _ragas ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ \noutput_filename = input_filename.replace('.xlsx', '_ragas.xlsx')\n\n# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ä¸€ç·’ã«ä¿å­˜ï¼‰\nsave_to_object_storage(df_for_ragas, config, bucket_name, output_filename, metadata_df=metadata_df)\nprint(f\"   âœ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"RAGASè©•ä¾¡å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ\")\nprint(f\"{'='*60}\\n\")\n\n# è©•ä¾¡çµæœã®ç¢ºèª\nprint(\"\\nè©•ä¾¡çµæœã‚µãƒ³ãƒ—ãƒ«:\")\nprint(df_for_ragas[['id', 'question', 'filter', 'answer_correctness', 'context_recall']].head())\n\n# è©•ä¾¡ã§ããªã‹ã£ãŸã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚Œã°è­¦å‘Š\nnull_count = df_for_ragas['answer_correctness'].isna().sum()\nif null_count > 0:\n    print(f\"\\nâš  è­¦å‘Š: {null_count}ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ã§è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸ\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}