{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a509e88-04a2-48f4-b6d8-f173c54e0016",
   "metadata": {},
   "outputs": [],
   "source": "####################################\n########  必要なライブラリ群 ##########\n####################################\n# 標準ライブラリ\nfrom typing import List, Dict, Any\nimport pandas as pd\nfrom datasets import Dataset\nfrom io import BytesIO\nimport time\nfrom datetime import datetime\n\n# Oracle Database\nimport oracledb\n\n# OCI\nimport oci\n\n# LangChain\nfrom langchain_community.embeddings import OCIGenAIEmbeddings\nfrom langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n\n# OCI GenAI Chat Model\nfrom oci.generative_ai_inference.models import (\n    CohereChatRequest,\n    OnDemandServingMode,\n    ChatDetails\n)\n\n# Reranker\nfrom sentence_transformers import CrossEncoder\nimport torch\n\n# RAGAS\nfrom ragas import evaluate\nfrom ragas.metrics import Faithfulness, AnswerCorrectness, ContextPrecision, ContextRecall\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\nfrom langchain_core.outputs import LLMResult\n\n# 設定読み込み（config_loader.pyと同じディレクトリに配置）\nfrom config_loader import (\n    load_config,\n    get_db_connection_params,\n    get_oci_config,\n    get_genai_config,\n    get_object_storage_config\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "#  定数及び変数  # \n",
    "################\n",
    "\n",
    "# .envファイルから環境変数を読み込み\n",
    "load_config()\n",
    "\n",
    "# 表名\n",
    "source_documents_table = \"source_documents\"\n",
    "chunks_table = \"chunks\"\n",
    "\n",
    "# Oracle Database接続\n",
    "db_params = get_db_connection_params()\n",
    "connection = oracledb.connect(**db_params)\n",
    "print(connection)\n",
    "\n",
    "# OCI設定\n",
    "config = get_oci_config()\n",
    "genai_config = get_genai_config()\n",
    "os_config = get_object_storage_config()\n",
    "\n",
    "# 設定値を変数に展開\n",
    "region = config.get('region', 'ap-osaka-1')\n",
    "compartment_id = genai_config['compartment_id']\n",
    "\n",
    "# FAQファイル用のObject Storage\n",
    "bucket_name = \"faq\"\n",
    "object_name = \"faq.xlsx\"\n",
    "\n",
    "# LLM\n",
    "embedding_model = \"cohere.embed-v4.0\"\n",
    "chat_model = \"cohere.command-a-03-2025\"\n",
    "rerank_model = \"hotchpotch/japanese-reranker-base-v2\"\n",
    "service_endpoint = f\"https://inference.generativeai.{region}.oci.oraclecloud.com\"\n",
    "\n",
    "print(f\"\\n✓ 設定読み込み完了\")\n",
    "print(f\"  - Region: {region}\")\n",
    "print(f\"  - Embedding Model: {embedding_model}\")\n",
    "print(f\"  - Chat Model: {chat_model}\")\n",
    "print(f\"  - Rerank Model: {rerank_model}\")\n",
    "print(f\"  - FAQ Bucket: {bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Object StorageからExcelファイルを取得 #\n",
    "######################################\n",
    "def load_excel_from_object_storage(config, bucket_name, object_name=\"faq.xlsx\", sheet_name=0):\n",
    "    \"\"\"\n",
    "    OCI Object StorageからExcelファイルを読み込み、DataFrameを返す\n",
    "    \n",
    "    Args:\n",
    "        config (dict): OCI設定情報\n",
    "        bucket_name (str): Object Storageのバケット名\n",
    "        object_name (str): Object Storageのオブジェクト名（デフォルト: \"faq.xlsx\"）\n",
    "        sheet_name (str or int): 読み込むシート名またはインデックス（デフォルト: 0）\n",
    "    \n",
    "    Returns:\n",
    "        df (DataFrame): データを含むDataFrame\n",
    "    \"\"\"\n",
    "    object_storage_client = oci.object_storage.ObjectStorageClient(config)    \n",
    "    namespace = object_storage_client.get_namespace().data #type: ignore\n",
    "    \n",
    "    # Object StorageからExcelファイルを取得\n",
    "    get_object_response = object_storage_client.get_object(namespace, bucket_name, object_name)\n",
    "    \n",
    "    # バイナリデータをExcelとして読み込み\n",
    "    excel_data = BytesIO(get_object_response.data.content) #type: ignore\n",
    "    df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "    \n",
    "    # 必要な列が存在するか確認（FAQファイルまたはResultsシートの場合のみ）\n",
    "    if sheet_name == 0 or sheet_name == 'Results':\n",
    "        required_columns = ['id', 'question', 'ground_truth', 'filter']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"必要な列が不足しています: {missing_columns}\")\n",
    "    \n",
    "    print(\"\\n✓ データの読み込みが完了しました\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d19ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 生成された回答をExcelとしてObject Storageに保存 #\n",
    "##############################################\n",
    "\n",
    "def save_to_object_storage(df, config, bucket_name, output_filename, metadata_df=None):\n",
    "    \"\"\"\n",
    "    DataFrameをExcelファイルとしてOCI Object Storageに保存\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): 保存するDataFrame\n",
    "        config (dict): OCI設定情報\n",
    "        bucket_name (str): Object Storageのバケット名\n",
    "        output_filename (str): 保存するファイル名（例: \"rag_results.xlsx\"）\n",
    "        metadata_df (DataFrame, optional): メタデータ用のDataFrame\n",
    "    \n",
    "    Returns:\n",
    "        str: アップロードしたファイルのURL情報\n",
    "    \"\"\"\n",
    "    excel_buffer = BytesIO()\n",
    "    \n",
    "    # ExcelWriterで複数シート対応\n",
    "    with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        if metadata_df is not None:\n",
    "            metadata_df.to_excel(writer, sheet_name='Settings', index=False)\n",
    "    \n",
    "    excel_buffer.seek(0)\n",
    "    \n",
    "    # Object Storageクライアントの作成\n",
    "    object_storage_client = oci.object_storage.ObjectStorageClient(config)\n",
    "    namespace = object_storage_client.get_namespace().data #type: ignore\n",
    "    \n",
    "    try:\n",
    "        # Object Storageにアップロード\n",
    "        object_storage_client.put_object(\n",
    "            namespace_name=namespace,\n",
    "            bucket_name=bucket_name,\n",
    "            object_name=output_filename,\n",
    "            put_object_body=excel_buffer.getvalue()\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Object Storageへのアップロード成功\")\n",
    "        print(f\"  バケット: {bucket_name}\")\n",
    "        print(f\"  ファイル名: {output_filename}\")\n",
    "        print(f\"  行数: {len(df)}\")\n",
    "        \n",
    "        return f\"Successfully uploaded to {bucket_name}/{output_filename}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ アップロードエラー: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Embeddingする #\n",
    "################\n",
    "\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=service_endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "\n",
    "embeddings = OCIGenAIEmbeddings(\n",
    "  model_id         = embedding_model,\n",
    "  service_endpoint = service_endpoint,\n",
    "  truncate         = \"NONE\",\n",
    "  compartment_id   = compartment_id,\n",
    "  auth_type        = \"API_KEY\",\n",
    "  client=generative_ai_inference_client\n",
    ")\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"\n",
    "    単一の文字列をEmbeddingしてベクトルデータを返す\n",
    "    \n",
    "    Args:\n",
    "        text (str): Embedding対象の文字列\n",
    "        \n",
    "    Returns:\n",
    "        embedding (str_vector): Embeddingベクトル文字列（浮動小数点数のリスト文字列）\n",
    "    \"\"\"\n",
    "    \n",
    "    # Embeddingの実行 (pythonのリスト形式): [0.1, 0.2, 0.3,...]\n",
    "    embedding_ = embeddings.embed_query(text)\n",
    "\n",
    "    # データベースへのロード用に型変換: [0.1, 0.2, 0.3,...] → \"[0.1, 0.2, 0.3,...]\"\n",
    "    embedding = str(embedding_)\n",
    "        \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7b6c8",
   "metadata": {},
   "outputs": [],
   "source": "##############\n# ベクトル検索 #\n##############\n\ndef vector_search(\n    query: str,\n    top_k: int = 5,\n    filtering: str | None = None\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    ベクトル類似度検索を実行（チャンク対応版）\n    \n    Args:\n        query (str): 検索クエリ文字列\n        top_k (int, optional): 返却する上位結果数。デフォルトは5\n        filtering (str, optional): ソース種別でフィルタリング。Noneの場合は全件を対象とする\n    \n    Returns:\n        List[Dict[str, Any]]: 検索結果のリスト\n    \"\"\"\n    # 1. クエリをベクトル化\n    query_vector = embed_text(query)\n    \n    # 2. データベース接続\n    connection = oracledb.connect(**db_params)\n    cursor = connection.cursor()\n    \n    try:\n        # 3. ベクトル類似度検索\n        if filtering:\n            sql = f\"\"\"\n            SELECT \n                c.chunk_id,\n                c.document_id,\n                s.filename,\n                c.chunk_text,\n                VECTOR_DISTANCE(c.embedding, :query_vector, COSINE) as distance\n            FROM {chunks_table} c\n            JOIN {source_documents_table} s ON c.document_id = s.document_id\n            WHERE s.filtering = :filtering\n            ORDER BY VECTOR_DISTANCE(c.embedding, :query_vector, COSINE)\n            FETCH FIRST :top_k ROWS ONLY\n            \"\"\"\n            \n            cursor.execute(sql, {\n                'query_vector': query_vector,\n                'filtering': filtering,\n                'top_k': top_k\n            })\n        else:\n            sql = f\"\"\"\n            SELECT \n                c.chunk_id,\n                c.document_id,\n                s.filename,\n                c.chunk_text,\n                VECTOR_DISTANCE(c.embedding, :query_vector, COSINE) as distance\n            FROM {chunks_table} c\n            JOIN {source_documents_table} s ON c.document_id = s.document_id\n            ORDER BY VECTOR_DISTANCE(c.embedding, :query_vector, COSINE)\n            FETCH FIRST :top_k ROWS ONLY\n            \"\"\"\n            \n            cursor.execute(sql, {\n                'query_vector': query_vector,\n                'top_k': top_k\n            })\n        \n        # 4. 結果を取得\n        results = []\n        for row in cursor:\n            chunk_text_clob = row[3]\n            chunk_text_content = chunk_text_clob.read() if chunk_text_clob is not None else \"\"\n            \n            results.append({\n                'chunk_id': row[0],\n                'document_id': row[1],\n                'filename': row[2],\n                'chunk_text': chunk_text_content,\n                'distance': float(row[4])\n            })\n        \n        return results\n    finally:\n        cursor.close()\n        connection.close()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Rerankerモデルの初期化 (v2) #\n",
    "####################################\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Rerankerモデル (japanese-reranker-base-v2) を初期化中...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"最適なデバイスを自動検出\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif hasattr(torch, \"mps\") and torch.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "device = detect_device()\n",
    "print(f\"✓ 検出デバイス: {device}\")\n",
    "\n",
    "print(\"✓ モデルをロード中...\")\n",
    "reranker_model = CrossEncoder(\n",
    "    'hotchpotch/japanese-reranker-base-v2',\n",
    "    max_length=512,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "if device in [\"cuda\", \"mps\"]:\n",
    "    print(\"✓ half精度に変換中...\")\n",
    "    reranker_model.model.half()\n",
    "\n",
    "print(f\"✓ Rerankerモデルの初期化完了\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Rerank #\n",
    "##########\n",
    "\n",
    "def rerank_chunks(query: str, chunks: List[Dict], top_n: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    hotchpotch/japanese-reranker-base-v2を使用してチャンクを再ランク付け\n",
    "    \n",
    "    Args:\n",
    "        query (str): ユーザーのクエリ文字列\n",
    "        chunks (List[Dict]): Vector Searchで取得したチャンクのリスト\n",
    "        top_n (int): 最終的に返すチャンク数\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: Rerankされたチャンクのリスト（rerank_scoreを含む）\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    # チャンクテキストのリストを作成\n",
    "    documents = [chunk['chunk_text'] for chunk in chunks]\n",
    "    \n",
    "    # クエリとドキュメントのペアを作成\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    \n",
    "    try:\n",
    "        # Rerankerでスコア計算\n",
    "        scores = reranker_model.predict(\n",
    "            pairs, \n",
    "            show_progress_bar=False,\n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        # 元のチャンクにrerankスコアを追加\n",
    "        for chunk, score in zip(chunks, scores):\n",
    "            chunk['rerank_score'] = float(score)\n",
    "        \n",
    "        # スコアでソート（降順）\n",
    "        chunks.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        # 上位top_n件を返す\n",
    "        return chunks[:top_n]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Rerankエラー: {e}\")\n",
    "        print(f\"⚠ Vector Searchの結果上位{top_n}件をそのまま返します\")\n",
    "        # Fallback: distanceでソート（小さい方が類似）\n",
    "        return sorted(chunks, key=lambda x: x.get('distance', float('inf')))[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72302485",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# 回答生成 #\n",
    "###########\n",
    "\n",
    "def generate_answer(\n",
    "    query: str,\n",
    "    contexts: str,\n",
    "    chat_model: str = \"cohere.command-a-03-2025\",\n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 0.3,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: int = 60,\n",
    "    answer_prompt: str = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    検索結果を元に回答を生成（Cohere専用）\n",
    "    \n",
    "    Args:\n",
    "        query (str): ユーザーの質問\n",
    "        contexts (str): 参考ドキュメントのテキスト（結合済み）\n",
    "        chat_model (str): 使用するモデルID。以下の2つが使えます。\n",
    "            - \"cohere.command-a-03-2025\" (デフォルト)\n",
    "            - \"cohere.command-r-plus-08-2024\"\n",
    "        max_tokens (int): 最大トークン数\n",
    "        temperature (float): 温度パラメータ（0-1、創造性の制御）\n",
    "        max_retries (int): HTTP 429発生時の最大リトライ回数。デフォルトは3\n",
    "        retry_delay (int): リトライ間隔の初期値（秒）。デフォルトは60秒\n",
    "        answer_prompt (str): 回答生成時の指示文\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: 生成結果の辞書。以下のキーを持つ:\n",
    "            - answer (str): 生成された回答テキスト\n",
    "            - model_used (str): 使用したモデルID\n",
    "    \"\"\"\n",
    "    # プロンプト作成\n",
    "    prompt = f\"\"\"以下のドキュメントを参考に、質問に回答してください。\n",
    "\n",
    "【参考ドキュメント】\n",
    "{contexts}\n",
    "\n",
    "【質問】\n",
    "{query}\n",
    "\n",
    "【回答】\n",
    "{answer_prompt}\n",
    "\"\"\"\n",
    "    \n",
    "    # リクエスト作成\n",
    "    chat_request = CohereChatRequest(\n",
    "        message=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        frequency_penalty=0,\n",
    "        top_p=0.75,\n",
    "        top_k=0\n",
    "    )\n",
    "    \n",
    "    chat_detail = ChatDetails(\n",
    "        serving_mode=OnDemandServingMode(model_id=chat_model),\n",
    "        compartment_id=compartment_id,\n",
    "        chat_request=chat_request\n",
    "    )\n",
    "    \n",
    "    # リトライロジック\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            # 回答生成実行\n",
    "            response = generative_ai_inference_client.chat(chat_detail)\n",
    "            \n",
    "            # レスポンスから回答を取得\n",
    "            answer = response.data.chat_response.text #type: ignore\n",
    "            \n",
    "            return {\n",
    "                'answer': answer,\n",
    "                'model_used': chat_model\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            # HTTP 429エラー（Rate Limit）の場合\n",
    "            if hasattr(e, 'status') and e.status == 429:\n",
    "                if attempt < max_retries:\n",
    "                    # 指数バックオフでリトライ間隔を延長\n",
    "                    wait_time = retry_delay * (2 ** attempt)\n",
    "                    print(f\"HTTP 429エラー発生。{wait_time}秒後にリトライします... (試行 {attempt + 1}/{max_retries})\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    # 最大リトライ回数に達した場合\n",
    "                    raise Exception(f\"最大リトライ回数({max_retries})に達しました。HTTP 429エラーが継続しています。\") from e\n",
    "            else:\n",
    "                # 429以外のエラーは即座に再送出\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532fad4",
   "metadata": {},
   "outputs": [],
   "source": "################\n# RAGAS評価関数 #\n################\n\ndef evaluate_with_ragas(question, answer, contexts, ground_truth):\n    \"\"\"\n    RAGシステムの出力をRAGASで評価\n    \n    Args:\n        question (List[str]): 質問のリスト\n        answer (List[str]): RAGシステムが生成した回答のリスト\n        contexts (List[List[str]]): 各質問に対して検索されたコンテキストのリスト（リストのリスト）\n        ground_truth (List[str]): 正解となる回答のリスト\n    \n    Returns:\n        result (dict): 評価結果\n            - faithfulness\n            - answer_relevancy\n            - context_precision\n            - context_recall\n    \"\"\"\n    # RAGAS評価専用のクライアント（タイムアウト延長版）\n    generative_ai_client_for_ragas = oci.generative_ai_inference.GenerativeAiInferenceClient(\n        config=config, \n        service_endpoint=service_endpoint, \n        retry_strategy=oci.retry.NoneRetryStrategy(), \n        timeout=(30, 600)  # 接続30秒、読み取り600秒に延長\n    )\n    \n    # LLMの準備\n    llm4eval = ChatOCIGenAI(\n        model_id=\"cohere.command-a-03-2025\",\n        service_endpoint=service_endpoint,\n        compartment_id=compartment_id,\n        is_stream=False,\n        model_kwargs={\"temperature\": 0.0, \"max_tokens\": 4000},\n        auth_type=\"API_KEY\",\n        client=generative_ai_client_for_ragas)  # タイムアウト延長版クライアントを使用\n\n    # 埋め込みモデルの準備\n    embeddings4eval = OCIGenAIEmbeddings(\n        model_id=embedding_model,\n        service_endpoint=service_endpoint,\n        compartment_id=compartment_id,\n        truncate=\"END\",\n        auth_type=\"API_KEY\",\n        client=generative_ai_client_for_ragas)  # タイムアウト延長版クライアントを使用\n\n    # データセットの作成\n    ds = Dataset.from_dict(\n        {\n            \"question\": question,\n            \"answer\": answer,\n            \"contexts\": contexts,\n            \"ground_truth\": ground_truth,\n        }\n    )\n\n    # OCI Generative AI Cohere Chat用のfinished_parserを実装\n    def my_finished_parser(response: LLMResult) -> bool:\n        if (response.generations \n            and response.generations[0] \n            and response.generations[0][0].generation_info \n            and response.generations[0][0].generation_info.get('finish_reason') == 'COMPLETE'):\n            return True\n        return False\n\n    # 評価用LLMのインスタンス生成（ここで、finished_parserを設定）\n    evaluator_llm = LangchainLLMWrapper(llm4eval, is_finished_parser=my_finished_parser)\n\n    # 評価用埋め込みモデルのインスタンス生成\n    evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings4eval)\n\n    # メトリクスのインスタンスを作成\n    metrics = [\n        Faithfulness(llm=evaluator_llm),\n        AnswerCorrectness(llm=evaluator_llm, embeddings=evaluator_embeddings),\n        ContextPrecision(llm=evaluator_llm),\n        ContextRecall(llm=evaluator_llm)\n]\n\n    # 評価の実行（エラー時は明示的に停止）\n    result = evaluate(ds, metrics, raise_exceptions=True)\n\n    return result"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e38140-44a2-4167-8124-e825ab1dc406",
   "metadata": {},
   "outputs": [],
   "source": "###################################\n# メイン処理1: Excelの質問への回答生成 #\n###################################\n\n# ベクトル検索のパラメータ\nTOP_K = 20  # ベクトル検索の上位何件を返却するか。Rerank無効時はこの値がコンテキスト件数\n\n# Rerankのパラメータ\nRERANK_ENABLED = True # True or False\nRERANK_TOP_N = 5  # Rerank後に上位何件をコンテキストとして利用するか\n\n# 回答生成時のパラメータ\nTEMPERATURE = 0.3\nANSWER_PROMPT = \"\"\"\n参考ドキュメントの情報に基づいて、正確に回答してください。ドキュメントに情報がない場合は、その旨を伝えてください。\n参考ドキュメントの情報を回答に利用した場合は、参考ドキュメントのファイル名も提示してください。\n\"\"\"\n\n# FAQ Excelファイルからデータロード\nfaq_df = load_excel_from_object_storage(config, bucket_name, object_name)\n\nprint(f\"FAQデータに対してRAG処理を開始します（全{len(faq_df)}件）\\n\")\n\n# answer, contexts, パフォーマンス計測用の列を新規作成\nfaq_df['answer'] = None\nfaq_df['contexts'] = None\nfaq_df['vector_search_time'] = 0.0\nfaq_df['rerank_time'] = 0.0\nfaq_df['generation_time'] = 0.0\nfaq_df['total_time'] = 0.0\n\n# 全体処理時間の計測開始\noverall_start_time = time.time()\n\n# 各質問に対して処理\nfor idx, row in faq_df.iterrows():\n    question = row['question']\n    filter_value = row['filter']\n    \n    if pd.notna(filter_value) and filter_value != \"\":\n        filtering = filter_value\n    else:\n        filtering = None\n    \n    print(f\"[{idx + 1}/{len(faq_df)}] 処理中: {question[:50]}...\")\n    \n    try:\n        # 1. ベクトル検索でコンテキストを取得（時間計測）\n        vector_search_start = time.time()\n        candidates = vector_search(\n            query=question,\n            top_k=TOP_K,\n            filtering=filtering\n        )\n        vector_search_time = time.time() - vector_search_start\n        faq_df.at[idx, 'vector_search_time'] = vector_search_time\n\n        # 2. Rerankingで絞り込み（時間計測）\n        rerank_start = time.time()\n        if RERANK_ENABLED:\n            search_results = rerank_chunks(question, candidates, top_n=RERANK_TOP_N)\n        else:\n            search_results = candidates[:RERANK_TOP_N]\n        rerank_time = time.time() - rerank_start\n        faq_df.at[idx, 'rerank_time'] = rerank_time\n \n        # 3. contextsを作成（LLMに渡す用 & DataFrame格納用で同一）\n        contexts = \"\\n\\n\".join([\n            f\"[ドキュメント {i+1}: {result['filename']}]\\n{result['chunk_text']}\"\n            for i, result in enumerate(search_results)\n        ])\n        \n        # 4. LLMで回答生成（時間計測）\n        generation_start = time.time()\n        result = generate_answer(\n            query=question,\n            contexts=contexts,\n            chat_model=chat_model,\n            temperature=TEMPERATURE,\n            answer_prompt=ANSWER_PROMPT\n        )\n        generation_time = time.time() - generation_start\n        faq_df.at[idx, 'generation_time'] = generation_time\n        \n        # 5. 合計時間を計算\n        total_time = vector_search_time + rerank_time + generation_time\n        faq_df.at[idx, 'total_time'] = total_time\n        \n        # 6. DataFrameに結果を格納\n        faq_df.at[idx, 'answer'] = result['answer']\n        faq_df.at[idx, 'contexts'] = contexts\n        \n        print(f\"  ✓ 完了 (検索結果: {len(search_results)}件, 処理時間: {total_time:.2f}秒)\")\n        print(f\"    - ベクトル検索: {vector_search_time:.2f}秒\")\n        print(f\"    - Rerank: {rerank_time:.2f}秒\")\n        print(f\"    - 回答生成: {generation_time:.2f}秒\")\n        \n    except Exception as e:\n        print(f\"  ✗ エラー: {e}\")\n        faq_df.at[idx, 'answer'] = \"\"\n        faq_df.at[idx, 'contexts'] = \"\"\n        continue\n\n# 全体処理時間の計測終了\noverall_end_time = time.time()\noverall_processing_time = overall_end_time - overall_start_time\n\nprint(f\"\\n{'='*60}\")\nprint(f\"✓ RAG処理が完了しました\")\nprint(f\"{'='*60}\")\nprint(f\"全体処理時間: {overall_processing_time:.2f}秒\")\nprint(f\"平均処理時間: {overall_processing_time/len(faq_df):.2f}秒/件\")\nprint(f\"\\n【処理時間の統計】\")\nprint(f\"  ベクトル検索平均: {faq_df['vector_search_time'].mean():.2f}秒\")\nprint(f\"  Rerank平均: {faq_df['rerank_time'].mean():.2f}秒\")\nprint(f\"  回答生成平均: {faq_df['generation_time'].mean():.2f}秒\")\nprint(f\"  合計平均: {faq_df['total_time'].mean():.2f}秒\")\n\n# 結果を確認\nprint(\"\\n処理結果:\")\nprint(faq_df[['id', 'question', 'ground_truth', 'filter', 'answer', 'total_time']].head())\n\n# メタデータを作成（パフォーマンス統計を含む）\nmetadata = {\n    'パラメータ': [\n        'TOP_K (ベクトル検索件数)',\n        'RERANK_ENABLED (Rerankが有効か)',\n        'RERANK_TOP_N (Rerank後件数)',\n        'TEMPERATURE (温度)',\n        'embedding_model',\n        'chat_model',\n        'rerank_model',\n        'service_endpoint',\n        '実行日時',\n        'FAQ件数',\n        '全体処理時間（秒）',\n        '平均処理時間/件（秒）',\n        'ベクトル検索平均時間（秒）',\n        'Rerank平均時間（秒）',\n        '回答生成平均時間（秒）'\n    ],\n    '設定値': [\n        TOP_K,\n        RERANK_ENABLED,\n        RERANK_TOP_N,\n        TEMPERATURE,\n        embedding_model,\n        chat_model,\n        rerank_model,\n        service_endpoint,\n        datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        len(faq_df),\n        f\"{overall_processing_time:.2f}\",\n        f\"{overall_processing_time/len(faq_df):.2f}\",\n        f\"{faq_df['vector_search_time'].mean():.2f}\",\n        f\"{faq_df['rerank_time'].mean():.2f}\",\n        f\"{faq_df['generation_time'].mean():.2f}\"\n    ]\n}\nmetadata_df = pd.DataFrame(metadata)\n\n# ファイル名生成（タイムスタンプのみ）\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\noutput_filename = f\"rag_result_{timestamp}.xlsx\"\n\n# Object Storageにアップロード（メタデータ付き）\nsave_to_object_storage(faq_df, config, bucket_name, output_filename, metadata_df=metadata_df)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1500db",
   "metadata": {},
   "outputs": [],
   "source": "#############################\n# メイン処理2: RAGAS評価の実行 #\n#############################\n\nprint(f\"\\n{'='*60}\")\nprint(f\"RAGAS評価処理を開始します\")\nprint(f\"{'='*60}\\n\")\n\n# 評価対象のファイル名を指定\ninput_filename = output_filename  # 個別で対象ファイルを指定する場合は、ここを修正してください\n\n# 1. Object StorageからExcelファイルをダウンロード\nprint(f\"1. Object Storageからファイルをダウンロード中...\")\ndf_for_ragas = load_excel_from_object_storage(config, bucket_name, input_filename, sheet_name='Results')\n\nprint(f\"   ✓ ダウンロード完了: {len(df_for_ragas)}件のデータ\")\n\n# 2. RAGAS評価用のデータを準備\nprint(f\"\\n2. RAGAS評価用データを準備中...\")\n\n# DataFrameから必要なデータを抽出\nquestions = df_for_ragas['question'].tolist()\nanswers = df_for_ragas['answer'].tolist()\nground_truths = df_for_ragas['ground_truth'].tolist()\n\n# contextsをリストのリストに変換（RAGASの要求形式）\n# 各ドキュメントを個別要素に分割する\ncontexts_list = []\nfor ctx in df_for_ragas['contexts'].tolist():\n    if pd.isna(ctx) or ctx == \"\":\n        contexts_list.append([\"\"])\n    else:\n        # \"[ドキュメント\" で始まる行でドキュメントを分割\n        lines = ctx.split('\\n')\n        individual_docs = []\n        current_doc = \"\"\n        \n        for line in lines:\n            if line.startswith('[ドキュメント'):\n                # 新しいドキュメントが始まる\n                if current_doc:\n                    individual_docs.append(current_doc.strip())\n                current_doc = line + '\\n'\n            else:\n                current_doc += line + '\\n'\n        \n        # 最後のドキュメントを追加\n        if current_doc:\n            individual_docs.append(current_doc.strip())\n        \n        if individual_docs:\n            contexts_list.append(individual_docs)\n        else:\n            # 分割できなかった場合は元のまま\n            contexts_list.append([ctx])\n\nprint(f\"   ✓ データ準備完了\")\n\n# 3. RAGAS評価を実行\nprint(f\"\\n3. RAGAS評価を実行中...\")\nprint(f\"   （この処理には数分かかる場合があります）\")\n\ntry:\n    ragas_result = evaluate_with_ragas(questions, answers, contexts_list, ground_truths)\n    \n    print(f\"\\n   ✓ RAGAS評価完了\")\n    \nexcept Exception as e:\n    print(f\"\\n   ✗ RAGAS評価エラー: {e}\")\n    raise\n\n# 4. 評価結果をDataFrameに追記\nprint(f\"\\n4. 評価結果をDataFrameに追記中...\")\n\nresult_df = ragas_result.to_pandas() #type: ignore\n\n# 各メトリクスをDataFrameに追加\ndf_for_ragas['faithfulness'] = result_df['faithfulness']\ndf_for_ragas['answer_correctness'] = result_df['answer_correctness']\ndf_for_ragas['context_precision'] = result_df['context_precision']\ndf_for_ragas['context_recall'] = result_df['context_recall']\n\nprint(f\"   ✓ 追記完了\")\n\n# 5. メタデータシートも読み込んで保持\nprint(f\"\\n5. メタデータを読み込み中...\")\nmetadata_df = load_excel_from_object_storage(config, bucket_name, input_filename, sheet_name='Settings')\nprint(f\"   ✓ メタデータ読み込み完了\")\n\n# 6. 更新したExcelをObject Storageにアップロード\nprint(f\"\\n6. 評価結果をObject Storageにアップロード中...\")\n\n# ファイル名に _ragas サフィックスを追加\noutput_filename = input_filename.replace('.xlsx', '_ragas.xlsx')\n\n# アップロード（メタデータも一緒に保存）\nsave_to_object_storage(df_for_ragas, config, bucket_name, output_filename, metadata_df=metadata_df)\nprint(f\"   ✓ アップロード完了\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"RAGAS評価処理が完了しました\")\nprint(f\"{'='*60}\\n\")\n\n# 評価結果の確認\nprint(\"\\n評価結果サンプル:\")\nprint(df_for_ragas[['id', 'question', 'filter', 'faithfulness', 'answer_correctness', 'context_precision', 'context_recall']].head())"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}