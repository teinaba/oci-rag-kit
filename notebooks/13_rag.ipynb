{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a509e88-04a2-48f4-b6d8-f173c54e0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "########  必要なライブラリ群 ##########\n",
    "####################################\n",
    "# 標準ライブラリ\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from io import BytesIO\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Oracle Database\n",
    "import oracledb\n",
    "\n",
    "# OCI\n",
    "import oci\n",
    "\n",
    "# LangChain\n",
    "from langchain_community.embeddings import OCIGenAIEmbeddings\n",
    "from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n",
    "\n",
    "# OCI GenAI Chat Model\n",
    "from oci.generative_ai_inference.models import (\n",
    "    CohereChatRequest,\n",
    "    GenericChatRequest,\n",
    "    OnDemandServingMode,\n",
    "    ChatDetails,\n",
    "    UserMessage,\n",
    "    TextContent\n",
    ")\n",
    "\n",
    "# Reranker\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "# RAGAS\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, AnswerCorrectness, ContextPrecision, ContextRecall\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "# 設定読み込み（config_loader.pyと同じディレクトリに配置）\n",
    "from config_loader import (\n",
    "    load_config,\n",
    "    get_db_connection_params,\n",
    "    get_oci_config,\n",
    "    get_genai_config,\n",
    "    get_object_storage_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "#  定数及び変数  # \n",
    "################\n",
    "\n",
    "# .envファイルから環境変数を読み込み\n",
    "load_config()\n",
    "\n",
    "# 表名\n",
    "source_documents_table = \"source_documents\"\n",
    "chunks_table = \"chunks\"\n",
    "\n",
    "# Oracle Database接続\n",
    "db_params = get_db_connection_params()\n",
    "connection = oracledb.connect(**db_params)\n",
    "print(connection)\n",
    "\n",
    "# OCI設定\n",
    "config = get_oci_config()\n",
    "genai_config = get_genai_config()\n",
    "os_config = get_object_storage_config()\n",
    "\n",
    "# 設定値を変数に展開\n",
    "region = config.get('region', 'ap-osaka-1')\n",
    "compartment_id = genai_config['compartment_id']\n",
    "\n",
    "# FAQファイル用のObject Storage\n",
    "bucket_name = \"faq\"\n",
    "object_name = \"faq.xlsx\"\n",
    "\n",
    "# LLM\n",
    "embedding_model = \"cohere.embed-v4.0\"\n",
    "chat_model = \"cohere.command-a-03-2025\"\n",
    "rerank_model = \"hotchpotch/japanese-reranker-base-v2\"\n",
    "service_endpoint = f\"https://inference.generativeai.{region}.oci.oraclecloud.com\"\n",
    "\n",
    "print(f\"\\n✓ 設定読み込み完了\")\n",
    "print(f\"  - Region: {region}\")\n",
    "print(f\"  - Embedding Model: {embedding_model}\")\n",
    "print(f\"  - Chat Model: {chat_model}\")\n",
    "print(f\"  - Rerank Model: {rerank_model}\")\n",
    "print(f\"  - FAQ Bucket: {bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Object StorageからExcelファイルを取得 #\n",
    "######################################\n",
    "def load_excel_from_object_storage(config, bucket_name, object_name=\"faq.xlsx\", sheet_name=0):\n",
    "    \"\"\"\n",
    "    OCI Object StorageからExcelファイルを読み込み、DataFrameを返す\n",
    "    \n",
    "    Args:\n",
    "        config (dict): OCI設定情報\n",
    "        bucket_name (str): Object Storageのバケット名\n",
    "        object_name (str): Object Storageのオブジェクト名（デフォルト: \"faq.xlsx\"）\n",
    "        sheet_name (str or int): 読み込むシート名またはインデックス（デフォルト: 0）\n",
    "    \n",
    "    Returns:\n",
    "        df (DataFrame): データを含むDataFrame\n",
    "    \"\"\"\n",
    "    object_storage_client = oci.object_storage.ObjectStorageClient(config)    \n",
    "    namespace = object_storage_client.get_namespace().data #type: ignore\n",
    "    \n",
    "    # Object StorageからExcelファイルを取得\n",
    "    get_object_response = object_storage_client.get_object(namespace, bucket_name, object_name)\n",
    "    \n",
    "    # バイナリデータをExcelとして読み込み\n",
    "    excel_data = BytesIO(get_object_response.data.content) #type: ignore\n",
    "    df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "    \n",
    "    # 必要な列が存在するか確認（FAQファイルまたはResultsシートの場合のみ）\n",
    "    if sheet_name == 0 or sheet_name == 'Results':\n",
    "        required_columns = ['id', 'question', 'ground_truth', 'filter']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"必要な列が不足しています: {missing_columns}\")\n",
    "    \n",
    "    print(\"\\n✓ データの読み込みが完了しました\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d19ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 生成された回答をExcelとしてObject Storageに保存 #\n",
    "##############################################\n",
    "\n",
    "def save_to_object_storage(df, config, bucket_name, output_filename, metadata_df=None):\n",
    "    \"\"\"\n",
    "    DataFrameをExcelファイルとしてOCI Object Storageに保存\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): 保存するDataFrame\n",
    "        config (dict): OCI設定情報\n",
    "        bucket_name (str): Object Storageのバケット名\n",
    "        output_filename (str): 保存するファイル名（例: \"rag_results.xlsx\"）\n",
    "        metadata_df (DataFrame, optional): メタデータ用のDataFrame\n",
    "    \n",
    "    Returns:\n",
    "        str: アップロードしたファイルのURL情報\n",
    "    \"\"\"\n",
    "    excel_buffer = BytesIO()\n",
    "    \n",
    "    # ExcelWriterで複数シート対応\n",
    "    with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        if metadata_df is not None:\n",
    "            metadata_df.to_excel(writer, sheet_name='Settings', index=False)\n",
    "    \n",
    "    excel_buffer.seek(0)\n",
    "    \n",
    "    # Object Storageクライアントの作成\n",
    "    object_storage_client = oci.object_storage.ObjectStorageClient(config)\n",
    "    namespace = object_storage_client.get_namespace().data #type: ignore\n",
    "    \n",
    "    try:\n",
    "        # Object Storageにアップロード\n",
    "        object_storage_client.put_object(\n",
    "            namespace_name=namespace,\n",
    "            bucket_name=bucket_name,\n",
    "            object_name=output_filename,\n",
    "            put_object_body=excel_buffer.getvalue()\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Object Storageへのアップロード成功\")\n",
    "        print(f\"  バケット: {bucket_name}\")\n",
    "        print(f\"  ファイル名: {output_filename}\")\n",
    "        print(f\"  行数: {len(df)}\")\n",
    "        \n",
    "        return f\"Successfully uploaded to {bucket_name}/{output_filename}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ アップロードエラー: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Embeddingする #\n",
    "################\n",
    "\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=service_endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "\n",
    "embeddings = OCIGenAIEmbeddings(\n",
    "  model_id         = embedding_model,\n",
    "  service_endpoint = service_endpoint,\n",
    "  truncate         = \"NONE\",\n",
    "  compartment_id   = compartment_id,\n",
    "  auth_type        = \"API_KEY\",\n",
    "  client=generative_ai_inference_client\n",
    ")\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"\n",
    "    単一の文字列をEmbeddingしてベクトルデータを返す\n",
    "    \n",
    "    Args:\n",
    "        text (str): Embedding対象の文字列\n",
    "        \n",
    "    Returns:\n",
    "        embedding (str_vector): Embeddingベクトル文字列（浮動小数点数のリスト文字列）\n",
    "    \"\"\"\n",
    "    \n",
    "    # Embeddingの実行 (pythonのリスト形式): [0.1, 0.2, 0.3,...]\n",
    "    embedding_ = embeddings.embed_query(text)\n",
    "\n",
    "    # データベースへのロード用に型変換: [0.1, 0.2, 0.3,...] → \"[0.1, 0.2, 0.3,...]\"\n",
    "    embedding = str(embedding_)\n",
    "        \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# ベクトル検索 #\n",
    "##############\n",
    "\n",
    "def vector_search(\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    filtering: str | None = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ベクトル類似度検索を実行（チャンク対応版）\n",
    "    \n",
    "    Args:\n",
    "        query (str): 検索クエリ文字列\n",
    "        top_k (int, optional): 返却する上位結果数。デフォルトは5\n",
    "        filtering (str, optional): ソース種別でフィルタリング。Noneの場合は全件を対象とする\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: 検索結果のリスト\n",
    "    \"\"\"\n",
    "    # 1. クエリをベクトル化\n",
    "    query_vector = embed_text(query)\n",
    "    \n",
    "    # 2. データベース接続\n",
    "    connection = oracledb.connect(**db_params)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    try:\n",
    "        # 3. ベクトル類似度検索\n",
    "        if filtering:\n",
    "            sql = f\"\"\"\n",
    "            SELECT \n",
    "                c.chunk_id,\n",
    "                c.document_id,\n",
    "                s.filename,\n",
    "                c.chunk_text,\n",
    "                VECTOR_DISTANCE(c.embedding, :query_vector, COSINE) as distance\n",
    "            FROM {chunks_table} c\n",
    "            JOIN {source_documents_table} s ON c.document_id = s.document_id\n",
    "            WHERE s.filtering = :filtering\n",
    "            ORDER BY VECTOR_DISTANCE(c.embedding, :query_vector, COSINE)\n",
    "            FETCH FIRST :top_k ROWS ONLY\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(sql, {\n",
    "                'query_vector': query_vector,\n",
    "                'filtering': filtering,\n",
    "                'top_k': top_k\n",
    "            })\n",
    "        else:\n",
    "            sql = f\"\"\"\n",
    "            SELECT \n",
    "                c.chunk_id,\n",
    "                c.document_id,\n",
    "                s.filename,\n",
    "                c.chunk_text,\n",
    "                VECTOR_DISTANCE(c.embedding, :query_vector, COSINE) as distance\n",
    "            FROM {chunks_table} c\n",
    "            JOIN {source_documents_table} s ON c.document_id = s.document_id\n",
    "            ORDER BY VECTOR_DISTANCE(c.embedding, :query_vector, COSINE)\n",
    "            FETCH FIRST :top_k ROWS ONLY\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(sql, {\n",
    "                'query_vector': query_vector,\n",
    "                'top_k': top_k\n",
    "            })\n",
    "        \n",
    "        # 4. 結果を取得\n",
    "        results = []\n",
    "        for row in cursor:\n",
    "            chunk_text_clob = row[3]\n",
    "            chunk_text_content = chunk_text_clob.read() if chunk_text_clob is not None else \"\"\n",
    "            \n",
    "            results.append({\n",
    "                'chunk_id': row[0],\n",
    "                'document_id': row[1],\n",
    "                'filename': row[2],\n",
    "                'chunk_text': chunk_text_content,\n",
    "                'distance': float(row[4])\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Rerankerモデルの初期化 (v2) #\n",
    "####################################\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Rerankerモデル (japanese-reranker-base-v2) を初期化中...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"最適なデバイスを自動検出\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif hasattr(torch, \"mps\") and torch.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "device = detect_device()\n",
    "print(f\"✓ 検出デバイス: {device}\")\n",
    "\n",
    "print(\"✓ モデルをロード中...\")\n",
    "reranker_model = CrossEncoder(\n",
    "    'hotchpotch/japanese-reranker-base-v2',\n",
    "    max_length=512,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "if device in [\"cuda\", \"mps\"]:\n",
    "    print(\"✓ half精度に変換中...\")\n",
    "    reranker_model.model.half()\n",
    "\n",
    "print(f\"✓ Rerankerモデルの初期化完了\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Rerank #\n",
    "##########\n",
    "\n",
    "def rerank_chunks(query: str, chunks: List[Dict], top_n: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    hotchpotch/japanese-reranker-base-v2を使用してチャンクを再ランク付け\n",
    "    \n",
    "    Args:\n",
    "        query (str): ユーザーのクエリ文字列\n",
    "        chunks (List[Dict]): Vector Searchで取得したチャンクのリスト\n",
    "        top_n (int): 最終的に返すチャンク数\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: Rerankされたチャンクのリスト（rerank_scoreを含む）\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "    \n",
    "    # チャンクテキストのリストを作成\n",
    "    documents = [chunk['chunk_text'] for chunk in chunks]\n",
    "    \n",
    "    # クエリとドキュメントのペアを作成\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    \n",
    "    try:\n",
    "        # Rerankerでスコア計算\n",
    "        scores = reranker_model.predict(\n",
    "            pairs, \n",
    "            show_progress_bar=False,\n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        # 元のチャンクにrerankスコアを追加\n",
    "        for chunk, score in zip(chunks, scores):\n",
    "            chunk['rerank_score'] = float(score)\n",
    "        \n",
    "        # スコアでソート（降順）\n",
    "        chunks.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        # 上位top_n件を返す\n",
    "        return chunks[:top_n]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Rerankエラー: {e}\")\n",
    "        print(f\"⚠ Vector Searchの結果上位{top_n}件をそのまま返します\")\n",
    "        # Fallback: distanceでソート（小さい方が類似）\n",
    "        return sorted(chunks, key=lambda x: x.get('distance', float('inf')))[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72302485",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# 回答生成 #\n",
    "###########\n",
    "\n",
    "def generate_answer(\n",
    "    query: str,\n",
    "    contexts: str,\n",
    "    chat_model: str = \"cohere.command-a-03-2025\",\n",
    "    max_tokens: int = 1000,\n",
    "    temperature: float = 0.3,\n",
    "    top_p: float = 0.75,\n",
    "    frequency_penalty: float = 0.0,\n",
    "    top_k: int = 0,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: int = 60,\n",
    "    answer_prompt: str = \"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    検索結果を元に回答を生成（全モデル対応）\n",
    "\n",
    "    Args:\n",
    "        query (str): ユーザーの質問\n",
    "        contexts (str): 参考ドキュメントのテキスト（結合済み）\n",
    "        chat_model (str): 使用するモデルID。以下のモデルが使えます:\n",
    "            【Cohere】\n",
    "            - \"cohere.command-a-03-2025\" (デフォルト)\n",
    "            - \"cohere.command-r-plus-08-2024\"\n",
    "            【Meta Llama】\n",
    "            - \"meta.llama-3.3-70b-instruct\"\n",
    "            【xAI Grok】\n",
    "            - \"xai.grok-4-fast-non-reasoning\" (RAG推奨)\n",
    "            - \"xai.grok-4-fast-reasoning\" (複雑な推論用)\n",
    "            - \"xai.grok-4\"\n",
    "            【Google Gemini】\n",
    "            - \"google.gemini-2.5-pro\"\n",
    "            - \"google.gemini-2.5-flash\"\n",
    "            - \"google.gemini-2.5-flash-lite\"\n",
    "            【OpenAI GPT-OSS】\n",
    "            - \"openai.gpt-oss-20b\"\n",
    "            - \"openai.gpt-oss-120b\"\n",
    "        max_tokens (int): 最大トークン数\n",
    "        temperature (float): 温度パラメータ（0-1、創造性の制御）\n",
    "        top_p (float): Nucleus samplingのパラメータ（0-1）\n",
    "        frequency_penalty (float): 頻度ペナルティ（Cohere用、0-1）\n",
    "        top_k (int): Top-Kサンプリング（Cohere用、0で無効）\n",
    "        max_retries (int): HTTP 429発生時の最大リトライ回数。デフォルトは3\n",
    "        retry_delay (int): リトライ間隔の初期値（秒）。デフォルトは60秒\n",
    "        answer_prompt (str): 回答生成時の指示文\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: 生成結果の辞書。以下のキーを持つ:\n",
    "            - answer (str): 生成された回答テキスト\n",
    "            - model_used (str): 使用したモデルID\n",
    "    \"\"\"\n",
    "    # プロンプト作成\n",
    "    prompt = f\"\"\"以下のドキュメントを参考に、質問に回答してください。\n",
    "\n",
    "【参考ドキュメント】\n",
    "{contexts}\n",
    "\n",
    "【質問】\n",
    "{query}\n",
    "\n",
    "【回答】\n",
    "{answer_prompt}\n",
    "\"\"\"\n",
    "\n",
    "    # Cohereモデル用（CohereChatRequest）\n",
    "    if \"cohere\" in chat_model.lower():\n",
    "        chat_request = CohereChatRequest(\n",
    "            message=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        chat_detail = ChatDetails(\n",
    "            serving_mode=OnDemandServingMode(model_id=chat_model),\n",
    "            compartment_id=compartment_id,\n",
    "            chat_request=chat_request\n",
    "        )\n",
    "\n",
    "        # リトライロジック\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                # 回答生成実行\n",
    "                response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "                # レスポンスから回答を取得\n",
    "                answer = response.data.chat_response.text #type: ignore\n",
    "\n",
    "                return {\n",
    "                    'answer': answer,\n",
    "                    'model_used': chat_model\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                # HTTP 429エラー（Rate Limit）の場合\n",
    "                if hasattr(e, 'status') and e.status == 429:\n",
    "                    if attempt < max_retries:\n",
    "                        # 指数バックオフでリトライ間隔を延長\n",
    "                        wait_time = retry_delay * (2 ** attempt)\n",
    "                        print(f\"HTTP 429エラー発生。{wait_time}秒後にリトライします... (試行 {attempt + 1}/{max_retries})\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        # 最大リトライ回数に達した場合\n",
    "                        raise Exception(f\"最大リトライ回数({max_retries})に達しました。HTTP 429エラーが継続しています。\") from e\n",
    "                else:\n",
    "                    # 429以外のエラーは即座に再送出\n",
    "                    raise\n",
    "\n",
    "    # その他のモデル用（GenericChatRequest）\n",
    "    # Llama, Grok, Gemini, OpenAI GPT-OSS など\n",
    "    else:\n",
    "        # UserMessageとTextContentを使った正しいフォーマット\n",
    "        messages = [\n",
    "            UserMessage(content=[TextContent(text=prompt)])\n",
    "        ]\n",
    "\n",
    "        chat_request = GenericChatRequest(\n",
    "            api_format=\"GENERIC\",\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "\n",
    "        chat_detail = ChatDetails(\n",
    "            serving_mode=OnDemandServingMode(model_id=chat_model),\n",
    "            compartment_id=compartment_id,\n",
    "            chat_request=chat_request\n",
    "        )\n",
    "\n",
    "        # リトライロジック\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                # 回答生成実行\n",
    "                response = generative_ai_inference_client.chat(chat_detail)\n",
    "\n",
    "                # レスポンスから回答を取得\n",
    "                # GenericChatRequestのレスポンスフォーマット\n",
    "                answer = response.data.chat_response.choices[0].message.content[0].text #type: ignore\n",
    "\n",
    "                return {\n",
    "                    'answer': answer,\n",
    "                    'model_used': chat_model\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                # HTTP 429エラー（Rate Limit）の場合\n",
    "                if hasattr(e, 'status') and e.status == 429:\n",
    "                    if attempt < max_retries:\n",
    "                        # 指数バックオフでリトライ間隔を延長\n",
    "                        wait_time = retry_delay * (2 ** attempt)\n",
    "                        print(f\"HTTP 429エラー発生。{wait_time}秒後にリトライします... (試行 {attempt + 1}/{max_retries})\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        # 最大リトライ回数に達した場合\n",
    "                        raise Exception(f\"最大リトライ回数({max_retries})に達しました。HTTP 429エラーが継続しています。\") from e\n",
    "                else:\n",
    "                    # 429以外のエラーは即座に再送出\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# RAGAS評価関数 #\n",
    "################\n",
    "\n",
    "def evaluate_with_ragas(question, answer, contexts, ground_truth):\n",
    "    \"\"\"\n",
    "    RAGシステムの出力をRAGASで評価\n",
    "    \n",
    "    Args:\n",
    "        question (List[str]): 質問のリスト\n",
    "        answer (List[str]): RAGシステムが生成した回答のリスト\n",
    "        contexts (List[List[str]]): 各質問に対して検索されたコンテキストのリスト（リストのリスト）\n",
    "        ground_truth (List[str]): 正解となる回答のリスト\n",
    "    \n",
    "    Returns:\n",
    "        result (dict): 評価結果\n",
    "            - faithfulness\n",
    "            - answer_relevancy\n",
    "            - context_precision\n",
    "            - context_recall\n",
    "    \"\"\"\n",
    "    # RAGAS評価専用のクライアント（タイムアウト延長版）\n",
    "    generative_ai_client_for_ragas = oci.generative_ai_inference.GenerativeAiInferenceClient(\n",
    "        config=config, \n",
    "        service_endpoint=service_endpoint, \n",
    "        retry_strategy=oci.retry.NoneRetryStrategy(), \n",
    "        timeout=(30, 600)  # 接続30秒、読み取り600秒に延長\n",
    "    )\n",
    "    \n",
    "    # LLMの準備\n",
    "    llm4eval = ChatOCIGenAI(\n",
    "        model_id=\"cohere.command-a-03-2025\",\n",
    "        service_endpoint=service_endpoint,\n",
    "        compartment_id=compartment_id,\n",
    "        is_stream=False,\n",
    "        model_kwargs={\"temperature\": 0.0, \"max_tokens\": 4000},\n",
    "        auth_type=\"API_KEY\",\n",
    "        client=generative_ai_client_for_ragas)  # タイムアウト延長版クライアントを使用\n",
    "\n",
    "    # 埋め込みモデルの準備\n",
    "    embeddings4eval = OCIGenAIEmbeddings(\n",
    "        model_id=embedding_model,\n",
    "        service_endpoint=service_endpoint,\n",
    "        compartment_id=compartment_id,\n",
    "        truncate=\"END\",\n",
    "        auth_type=\"API_KEY\",\n",
    "        client=generative_ai_client_for_ragas)  # タイムアウト延長版クライアントを使用\n",
    "\n",
    "    # データセットの作成\n",
    "    ds = Dataset.from_dict(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"contexts\": contexts,\n",
    "            \"ground_truth\": ground_truth,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # OCI Generative AI Cohere Chat用のfinished_parserを実装\n",
    "    def my_finished_parser(response: LLMResult) -> bool:\n",
    "        if (response.generations \n",
    "            and response.generations[0] \n",
    "            and response.generations[0][0].generation_info \n",
    "            and response.generations[0][0].generation_info.get('finish_reason') == 'COMPLETE'):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # 評価用LLMのインスタンス生成（ここで、finished_parserを設定）\n",
    "    evaluator_llm = LangchainLLMWrapper(llm4eval, is_finished_parser=my_finished_parser)\n",
    "\n",
    "    # 評価用埋め込みモデルのインスタンス生成\n",
    "    evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings4eval)\n",
    "\n",
    "    # メトリクスのインスタンスを作成\n",
    "    metrics = [\n",
    "        Faithfulness(llm=evaluator_llm),\n",
    "        AnswerCorrectness(llm=evaluator_llm, embeddings=evaluator_embeddings),\n",
    "        ContextPrecision(llm=evaluator_llm),\n",
    "        ContextRecall(llm=evaluator_llm)\n",
    "]\n",
    "\n",
    "    # 評価の実行（エラー時は明示的に停止）\n",
    "    result = evaluate(ds, metrics, raise_exceptions=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e38140-44a2-4167-8124-e825ab1dc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# メイン処理1: Excelの質問への回答生成 #\n",
    "###################################\n",
    "\n",
    "# ベクトル検索のパラメータ\n",
    "TOP_K = 10  # ベクトル検索の上位何件を返却するか。Rerank無効時はこの値がコンテキスト件数\n",
    "\n",
    "# Rerankのパラメータ (RerankはCPU処理性能に依っては長時間要するため注意)\n",
    "RERANK_ENABLED = True # True or False\n",
    "RERANK_TOP_N = 5  # Rerank後に上位何件をコンテキストとして利用するか\n",
    "\n",
    "# LLM選択パラメータ\n",
    "# 以下から選択してください：\n",
    "# 【Cohere】 \"cohere.command-a-03-2025\", \"cohere.command-r-plus-08-2024\"\n",
    "# 【Meta Llama】 \"meta.llama-3.3-70b-instruct\"\n",
    "# 【xAI Grok】 \"xai.grok-4-fast-non-reasoning\", \"xai.grok-4-fast-reasoning\", \"xai.grok-4\"\n",
    "# 【Google Gemini】 \"google.gemini-2.5-pro\", \"google.gemini-2.5-flash\", \"google.gemini-2.5-flash-lite\"\n",
    "# 【OpenAI GPT-OSS】 \"openai.gpt-oss-20b\", \"openai.gpt-oss-120b\"\n",
    "CHAT_MODEL = \"cohere.command-a-03-2025\"  # 使用するLLMモデルを指定\n",
    "\n",
    "# LLMチューニングパラメータ\n",
    "MAX_TOKENS = 1000  # 生成する最大トークン数\n",
    "TEMPERATURE = 0.3  # 温度パラメータ（0-1、創造性の制御。0に近いほど決定的、1に近いほどランダム）\n",
    "TOP_P = 0.75  # Nucleus samplingのパラメータ（0-1）\n",
    "FREQUENCY_PENALTY = 0.0  # 頻度ペナルティ（Cohere用、0-1。繰り返しを抑制）\n",
    "TOP_K_SAMPLING = 0  # Top-Kサンプリング（Cohere用、0で無効）\n",
    "\n",
    "# 回答生成時のプロンプト\n",
    "ANSWER_PROMPT = \"\"\"\n",
    "参考ドキュメントの情報に基づいて、正確に回答してください。ドキュメントに情報がない場合は、その旨を伝えてください。\n",
    "回答は簡潔に平文で記載してください。\n",
    "\"\"\"\n",
    "\n",
    "# FAQ Excelファイルからデータロード\n",
    "faq_df = load_excel_from_object_storage(config, bucket_name, object_name)\n",
    "\n",
    "print(f\"FAQデータに対してRAG処理を開始します（全{len(faq_df)}件）\")\n",
    "print(f\"使用モデル: {CHAT_MODEL}\\n\")\n",
    "\n",
    "# answer, contexts, パフォーマンス計測用の列を新規作成\n",
    "faq_df['answer'] = None\n",
    "faq_df['contexts'] = None\n",
    "faq_df['vector_search_time'] = 0.0\n",
    "faq_df['rerank_time'] = 0.0\n",
    "faq_df['generation_time'] = 0.0\n",
    "faq_df['total_time'] = 0.0\n",
    "\n",
    "# 全体処理時間の計測開始\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# 各質問に対して処理\n",
    "for idx, row in faq_df.iterrows():\n",
    "    question = row['question']\n",
    "    filter_value = row['filter']\n",
    "    \n",
    "    if pd.notna(filter_value) and filter_value != \"\":\n",
    "        filtering = filter_value\n",
    "    else:\n",
    "        filtering = None\n",
    "    \n",
    "    print(f\"[{idx + 1}/{len(faq_df)}] 処理中: {question[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. ベクトル検索でコンテキストを取得（時間計測）\n",
    "        vector_search_start = time.time()\n",
    "        candidates = vector_search(\n",
    "            query=question,\n",
    "            top_k=TOP_K,\n",
    "            filtering=filtering\n",
    "        )\n",
    "        vector_search_time = time.time() - vector_search_start\n",
    "        faq_df.at[idx, 'vector_search_time'] = vector_search_time\n",
    "\n",
    "        # 2. Rerankingで絞り込み（時間計測）\n",
    "        rerank_start = time.time()\n",
    "        if RERANK_ENABLED:\n",
    "            search_results = rerank_chunks(question, candidates, top_n=RERANK_TOP_N)\n",
    "        else:\n",
    "            search_results = candidates[:RERANK_TOP_N]\n",
    "        rerank_time = time.time() - rerank_start\n",
    "        faq_df.at[idx, 'rerank_time'] = rerank_time\n",
    " \n",
    "        # 3. contextsを作成（LLMに渡す用 & DataFrame格納用で同一）\n",
    "        contexts = \"\\n\\n\".join([\n",
    "            f\"[ドキュメント {i+1}: {result['filename']}]\\n{result['chunk_text']}\"\n",
    "            for i, result in enumerate(search_results)\n",
    "        ])\n",
    "        \n",
    "        # 4. LLMで回答生成（時間計測）\n",
    "        generation_start = time.time()\n",
    "        result = generate_answer(\n",
    "            query=question,\n",
    "            contexts=contexts,\n",
    "            chat_model=CHAT_MODEL,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            frequency_penalty=FREQUENCY_PENALTY,\n",
    "            top_k=TOP_K_SAMPLING,\n",
    "            answer_prompt=ANSWER_PROMPT\n",
    "        )\n",
    "        generation_time = time.time() - generation_start\n",
    "        faq_df.at[idx, 'generation_time'] = generation_time\n",
    "        \n",
    "        # 5. 合計時間を計算\n",
    "        total_time = vector_search_time + rerank_time + generation_time\n",
    "        faq_df.at[idx, 'total_time'] = total_time\n",
    "        \n",
    "        # 6. DataFrameに結果を格納\n",
    "        faq_df.at[idx, 'answer'] = result['answer']\n",
    "        faq_df.at[idx, 'contexts'] = contexts\n",
    "        \n",
    "        print(f\"  ✓ 完了 (検索結果: {len(search_results)}件, 処理時間: {total_time:.2f}秒)\")\n",
    "        print(f\"    - ベクトル検索: {vector_search_time:.2f}秒\")\n",
    "        print(f\"    - Rerank: {rerank_time:.2f}秒\")\n",
    "        print(f\"    - 回答生成: {generation_time:.2f}秒\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ エラー: {e}\")\n",
    "        faq_df.at[idx, 'answer'] = \"\"\n",
    "        faq_df.at[idx, 'contexts'] = \"\"\n",
    "        continue\n",
    "\n",
    "# 全体処理時間の計測終了\n",
    "overall_end_time = time.time()\n",
    "overall_processing_time = overall_end_time - overall_start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ RAG処理が完了しました\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"全体処理時間: {overall_processing_time:.2f}秒\")\n",
    "print(f\"平均処理時間: {overall_processing_time/len(faq_df):.2f}秒/件\")\n",
    "print(f\"\\n【処理時間の統計】\")\n",
    "print(f\"  ベクトル検索平均: {faq_df['vector_search_time'].mean():.2f}秒\")\n",
    "print(f\"  Rerank平均: {faq_df['rerank_time'].mean():.2f}秒\")\n",
    "print(f\"  回答生成平均: {faq_df['generation_time'].mean():.2f}秒\")\n",
    "print(f\"  合計平均: {faq_df['total_time'].mean():.2f}秒\")\n",
    "\n",
    "# 結果を確認\n",
    "print(\"\\n処理結果:\")\n",
    "print(faq_df[['id', 'question', 'ground_truth', 'filter', 'answer', 'total_time']].head())\n",
    "\n",
    "# メタデータを作成（パフォーマンス統計を含む）\n",
    "metadata = {\n",
    "    'パラメータ': [\n",
    "        'TOP_K (ベクトル検索件数)',\n",
    "        'RERANK_ENABLED (Rerankが有効か)',\n",
    "        'RERANK_TOP_N (Rerank後件数)',\n",
    "        'CHAT_MODEL (使用LLMモデル)',\n",
    "        'MAX_TOKENS (最大トークン数)',\n",
    "        'TEMPERATURE (温度)',\n",
    "        'TOP_P (Nucleus sampling)',\n",
    "        'FREQUENCY_PENALTY (頻度ペナルティ)',\n",
    "        'TOP_K_SAMPLING (Top-K sampling)',\n",
    "        'ANSWER_PROMPT (回答生成時の指示文)',\n",
    "        'embedding_model',\n",
    "        'rerank_model',\n",
    "        'service_endpoint',\n",
    "        '実行日時',\n",
    "        'FAQ件数',\n",
    "        '全体処理時間（秒）',\n",
    "        '平均処理時間/件（秒）',\n",
    "        'ベクトル検索平均時間（秒）',\n",
    "        'Rerank平均時間（秒）',\n",
    "        '回答生成平均時間（秒）'\n",
    "    ],\n",
    "    '設定値': [\n",
    "        TOP_K,\n",
    "        RERANK_ENABLED,\n",
    "        RERANK_TOP_N,\n",
    "        CHAT_MODEL,\n",
    "        MAX_TOKENS,\n",
    "        TEMPERATURE,\n",
    "        TOP_P,\n",
    "        FREQUENCY_PENALTY,\n",
    "        TOP_K_SAMPLING,\n",
    "        ANSWER_PROMPT,\n",
    "        embedding_model,\n",
    "        rerank_model,\n",
    "        service_endpoint,\n",
    "        datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        len(faq_df),\n",
    "        f\"{overall_processing_time:.2f}\",\n",
    "        f\"{overall_processing_time/len(faq_df):.2f}\",\n",
    "        f\"{faq_df['vector_search_time'].mean():.2f}\",\n",
    "        f\"{faq_df['rerank_time'].mean():.2f}\",\n",
    "        f\"{faq_df['generation_time'].mean():.2f}\"\n",
    "    ]\n",
    "}\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "\n",
    "# ファイル名生成（タイムスタンプのみ）\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_filename = f\"rag_result_{timestamp}.xlsx\"\n",
    "\n",
    "# Object Storageにアップロード（メタデータ付き）\n",
    "save_to_object_storage(faq_df, config, bucket_name, output_filename, metadata_df=metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1500db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# メイン処理2: RAGAS評価の実行 #\n",
    "#############################\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RAGAS評価処理を開始します\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 評価対象のファイル名を指定\n",
    "input_filename = output_filename  # 個別で対象ファイルを指定する場合は、ここを修正してください\n",
    "\n",
    "# 1. Object StorageからExcelファイルをダウンロード\n",
    "print(f\"1. Object Storageからファイルをダウンロード中...\")\n",
    "df_for_ragas = load_excel_from_object_storage(config, bucket_name, input_filename, sheet_name='Results')\n",
    "\n",
    "print(f\"   ✓ ダウンロード完了: {len(df_for_ragas)}件のデータ\")\n",
    "\n",
    "# 2. RAGAS評価用のデータを準備\n",
    "print(f\"\\n2. RAGAS評価用データを準備中...\")\n",
    "\n",
    "# DataFrameから必要なデータを抽出\n",
    "questions = df_for_ragas['question'].tolist()\n",
    "answers = df_for_ragas['answer'].tolist()\n",
    "ground_truths = df_for_ragas['ground_truth'].tolist()\n",
    "\n",
    "# contextsをリストのリストに変換（RAGASの要求形式）\n",
    "# 各ドキュメントを個別要素に分割する\n",
    "contexts_list = []\n",
    "for ctx in df_for_ragas['contexts'].tolist():\n",
    "    if pd.isna(ctx) or ctx == \"\":\n",
    "        contexts_list.append([\"\"])\n",
    "    else:\n",
    "        # \"[ドキュメント\" で始まる行でドキュメントを分割\n",
    "        lines = ctx.split('\\n')\n",
    "        individual_docs = []\n",
    "        current_doc = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith('[ドキュメント'):\n",
    "                # 新しいドキュメントが始まる\n",
    "                if current_doc:\n",
    "                    individual_docs.append(current_doc.strip())\n",
    "                current_doc = line + '\\n'\n",
    "            else:\n",
    "                current_doc += line + '\\n'\n",
    "        \n",
    "        # 最後のドキュメントを追加\n",
    "        if current_doc:\n",
    "            individual_docs.append(current_doc.strip())\n",
    "        \n",
    "        if individual_docs:\n",
    "            contexts_list.append(individual_docs)\n",
    "        else:\n",
    "            # 分割できなかった場合は元のまま\n",
    "            contexts_list.append([ctx])\n",
    "\n",
    "print(f\"   ✓ データ準備完了\")\n",
    "\n",
    "# 3. RAGAS評価を実行\n",
    "print(f\"\\n3. RAGAS評価を実行中...\")\n",
    "print(f\"   （この処理には数分かかる場合があります）\")\n",
    "\n",
    "try:\n",
    "    ragas_result = evaluate_with_ragas(questions, answers, contexts_list, ground_truths)\n",
    "    \n",
    "    print(f\"\\n   ✓ RAGAS評価完了\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n   ✗ RAGAS評価エラー: {e}\")\n",
    "    raise\n",
    "\n",
    "# 4. 評価結果をDataFrameに追記\n",
    "print(f\"\\n4. 評価結果をDataFrameに追記中...\")\n",
    "\n",
    "result_df = ragas_result.to_pandas() #type: ignore\n",
    "\n",
    "# 各メトリクスをDataFrameに追加\n",
    "df_for_ragas['faithfulness'] = result_df['faithfulness']\n",
    "df_for_ragas['answer_correctness'] = result_df['answer_correctness']\n",
    "df_for_ragas['context_precision'] = result_df['context_precision']\n",
    "df_for_ragas['context_recall'] = result_df['context_recall']\n",
    "\n",
    "print(f\"   ✓ 追記完了\")\n",
    "\n",
    "# 5. メタデータシートも読み込んで保持\n",
    "print(f\"\\n5. メタデータを読み込み中...\")\n",
    "metadata_df = load_excel_from_object_storage(config, bucket_name, input_filename, sheet_name='Settings')\n",
    "print(f\"   ✓ メタデータ読み込み完了\")\n",
    "\n",
    "# 6. 更新したExcelをObject Storageにアップロード\n",
    "print(f\"\\n6. 評価結果をObject Storageにアップロード中...\")\n",
    "\n",
    "# ファイル名に _ragas サフィックスを追加\n",
    "output_filename = input_filename.replace('.xlsx', '_ragas.xlsx')\n",
    "\n",
    "# アップロード（メタデータも一緒に保存）\n",
    "save_to_object_storage(df_for_ragas, config, bucket_name, output_filename, metadata_df=metadata_df)\n",
    "print(f\"   ✓ アップロード完了\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RAGAS評価処理が完了しました\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# 評価結果の確認\n",
    "print(\"\\n評価結果サンプル:\")\n",
    "print(df_for_ragas[['id', 'question', 'filter', 'faithfulness', 'answer_correctness', 'context_precision', 'context_recall']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
